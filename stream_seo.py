import streamlit as st
import requests
import pandas as pd
import time
import json
import os
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from io import BytesIO
import random

# ============ é é¢è¨­å®š ============

st.set_page_config(
    page_title="SEO æ’åè¿½è¹¤å·¥å…· Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============ è‡ªè¨‚ CSS ============

st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
        text-align: center;
    }

    .debug-box {
        background: #1a1a2e;
        color: #00ff88;
        padding: 1rem;
        border-radius: 8px;
        font-family: monospace;
        font-size: 0.85rem;
        max-height: 300px;
        overflow-y: auto;
    }

    .stat-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .keyword-group-card {
        background: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 0.5rem;
    }
    
    .copy-box {
        background: #f1f5f9;
        border: 1px solid #cbd5e1;
        border-radius: 8px;
        padding: 0.75rem;
        font-family: monospace;
        font-size: 0.85rem;
        white-space: pre-wrap;
        max-height: 200px;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)

# ============ æ•¸æ“šå„²å­˜åŠŸèƒ½ ============

DATA_FILE = "serp_history.json"
KEYWORD_GROUPS_FILE = "keyword_groups.json"


def load_history():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {"records": [], "settings": {}}
    return {"records": [], "settings": {}}


def save_history(data):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_keyword_groups():
    """è¼‰å…¥é—œéµå­—çµ„"""
    if os.path.exists(KEYWORD_GROUPS_FILE):
        try:
            with open(KEYWORD_GROUPS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_keyword_groups(groups):
    """å„²å­˜é—œéµå­—çµ„"""
    with open(KEYWORD_GROUPS_FILE, "w", encoding="utf-8") as f:
        json.dump(groups, f, ensure_ascii=False, indent=2)


def add_record(history, record):
    record["timestamp"] = datetime.now().isoformat()
    record["date"] = datetime.now().strftime("%Y-%m-%d")
    record["time"] = datetime.now().strftime("%H:%M:%S")
    record["id"] = f"{record['date']}_{record['time'].replace(':', '')}"
    history["records"].append(record)
    save_history(history)
    return history


def export_single_record(record):
    """åŒ¯å‡ºå–®ä¸€è¨˜éŒ„ç‚º Excel"""
    output = BytesIO()

    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        df_rankings = pd.DataFrame(record.get("rankings", []))
        df_rankings.to_excel(writer, sheet_name="æ’å", index=False)

        info_data = {
            "é …ç›®": ["æ—¥æœŸ", "æ™‚é–“", "åœ°å€", "æˆ‘çš„ç¶²ç«™", "ç«¶çˆ­å°æ‰‹"],
            "å…§å®¹": [
                record.get("date", ""),
                record.get("time", ""),
                record.get("region", ""),
                ", ".join(record.get("my_sites", [])),
                ", ".join(record.get("competitors", []))
            ]
        }
        pd.DataFrame(info_data).to_excel(writer, sheet_name="æŸ¥è©¢è³‡è¨Š", index=False)

    output.seek(0)
    return output


def normalize_domain(domain):
    """æ¨™æº–åŒ–ç¶²åŸŸåç¨±ï¼Œç§»é™¤ http/https å’Œå°¾éƒ¨æ–œç·š"""
    domain = domain.lower().strip()
    domain = domain.replace("https://", "").replace("http://", "")
    domain = domain.rstrip("/")
    # ç§»é™¤ www. å‰ç¶´
    if domain.startswith("www."):
        domain = domain[4:]
    return domain


def get_unique_sites(sites_list):
    """ç²å–å”¯ä¸€çš„ç¶²ç«™åˆ—è¡¨ï¼ˆæ¨™æº–åŒ–å¾Œï¼‰"""
    seen = {}
    unique = []
    for site in sites_list:
        normalized = normalize_domain(site)
        if normalized not in seen:
            seen[normalized] = site
            unique.append(site)
    return unique


def get_record_display_name(record):
    """ç²å–è¨˜éŒ„çš„é¡¯ç¤ºåç¨±"""
    date = record.get("date", "æœªçŸ¥")
    time_str = record.get("time", "")
    keyword_count = len(record.get("rankings", []))
    return f"{date} {time_str} ({keyword_count}å€‹é—œéµå­—)"


def get_all_sites_from_record(record):
    """å¾è¨˜éŒ„ä¸­ç²å–æ‰€æœ‰ç¶²ç«™ï¼ˆæ¨™æº–åŒ–å¾Œå»é‡ï¼‰"""
    all_sites = []
    seen = set()
    
    for site in record.get("my_sites", []):
        normalized = normalize_domain(site)
        if normalized not in seen:
            seen.add(normalized)
            all_sites.append(site)
    
    for site in record.get("competitors", []):
        normalized = normalize_domain(site)
        if normalized not in seen:
            seen.add(normalized)
            all_sites.append(site)
    
    return all_sites


def get_keyword_order_map(record):
    """ç²å–é—œéµå­—çš„åŸå§‹é †åºæ˜ å°„"""
    keywords = record.get("keywords", [])
    return {kw: idx for idx, kw in enumerate(keywords)}


def analyze_keyword_competition(rankings, site_a, site_b, keyword_order_map=None):
    """åˆ†æå…©å€‹ç¶²ç«™ä¹‹é–“çš„é—œéµå­—ç«¶çˆ­"""
    winning = []  # A è´
    losing = []  # A è¼¸
    both_ranked = []  # é›™æ–¹éƒ½æœ‰æ’å
    only_a = []  # åªæœ‰ A æœ‰æ’å
    only_b = []  # åªæœ‰ B æœ‰æ’å
    neither = []  # éƒ½æ²’æ’å

    site_a_normalized = normalize_domain(site_a)
    site_b_normalized = normalize_domain(site_b)

    for item in rankings:
        keyword = item.get("keyword")
        
        # æŸ¥æ‰¾ site_a çš„æ’å
        rank_a = None
        for key in item.keys():
            if key != "keyword" and normalize_domain(key) == site_a_normalized:
                rank_a = item.get(key)
                break
        
        # æŸ¥æ‰¾ site_b çš„æ’å
        rank_b = None
        for key in item.keys():
            if key != "keyword" and normalize_domain(key) == site_b_normalized:
                rank_b = item.get(key)
                break

        # ç²å–åŸå§‹é †åºï¼ˆç”¨æ–¼æ’åºï¼‰
        order = keyword_order_map.get(keyword, 9999) if keyword_order_map else 0

        if rank_a is None and rank_b is None:
            neither.append({"keyword": keyword, "order": order})
        elif rank_a is None:
            only_b.append({"keyword": keyword, "rank_b": rank_b, "order": order})
        elif rank_b is None:
            only_a.append({"keyword": keyword, "rank_a": rank_a, "order": order})
        else:
            both_ranked.append({
                "keyword": keyword,
                "rank_a": rank_a,
                "rank_b": rank_b,
                "diff": rank_b - rank_a,
                "order": order
            })
            if rank_a < rank_b:
                winning.append({"keyword": keyword, "rank_a": rank_a, "rank_b": rank_b, "order": order})
            elif rank_a > rank_b:
                losing.append({"keyword": keyword, "rank_a": rank_a, "rank_b": rank_b, "order": order})

    # æŒ‰åŸå§‹è¼¸å…¥é †åºæ’åº
    winning.sort(key=lambda x: x["order"])
    losing.sort(key=lambda x: x["order"])
    only_a.sort(key=lambda x: x["order"])
    only_b.sort(key=lambda x: x["order"])
    neither.sort(key=lambda x: x["order"])
    both_ranked.sort(key=lambda x: x["order"])

    return {
        "winning": winning,
        "losing": losing,
        "both_ranked": both_ranked,
        "only_a": only_a,
        "only_b": only_b,
        "neither": neither
    }


def analyze_site_keywords_detail(rankings, site, warning_threshold=20, keyword_order_map=None):
    """åˆ†æå–®ä¸€ç¶²ç«™çš„é—œéµå­—è©³æƒ…"""
    site_normalized = normalize_domain(site)
    
    details = {
        "top3": [],
        "top10": [],
        "top20": [],
        "top30": [],
        "warning": [],
        "na": []
    }

    for item in rankings:
        keyword = item.get("keyword")
        order = keyword_order_map.get(keyword, 9999) if keyword_order_map else 0
        
        # æŸ¥æ‰¾è©²ç¶²ç«™çš„æ’å
        rank = None
        for key in item.keys():
            if key != "keyword" and normalize_domain(key) == site_normalized:
                rank = item.get(key)
                break

        if rank is None:
            details["na"].append({"keyword": keyword, "order": order})
        else:
            if rank <= 3:
                details["top3"].append({"keyword": keyword, "rank": rank, "order": order})
            elif rank <= 10:
                details["top10"].append({"keyword": keyword, "rank": rank, "order": order})
            elif rank <= 20:
                details["top20"].append({"keyword": keyword, "rank": rank, "order": order})
            elif rank <= 30:
                details["top30"].append({"keyword": keyword, "rank": rank, "order": order})

            if rank > warning_threshold:
                details["warning"].append({"keyword": keyword, "rank": rank, "order": order})

    # æŒ‰åŸå§‹è¼¸å…¥é †åºæ’åº
    for key in details:
        details[key].sort(key=lambda x: x["order"])

    return details


# ============ ç©©å®šç‰ˆç•°æ­¥æœå°‹å¼•æ“ ============

class StableSerpSearcher:
    """ç©©å®šç‰ˆ SERP æœå°‹å™¨"""

    def __init__(self, api_key, region="hk", lang="zh-tw", max_concurrent=10,
                 delay_between_requests=0.1, max_retries=3, autocorrect=False):
        self.api_key = api_key
        self.region = region
        self.lang = lang
        self.max_concurrent = max_concurrent
        self.delay = delay_between_requests
        self.max_retries = max_retries
        self.autocorrect = autocorrect
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

    def log(self, message):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        log_entry = f"[{timestamp}] {message}"
        self.debug_logs.append(log_entry)

    async def _fetch_with_retry(self, session, keyword, page, semaphore):
        async with semaphore:
            await asyncio.sleep(random.uniform(0.05, self.delay))

            url = "https://google.serper.dev/search"
            payload = {
                "q": keyword,
                "gl": self.region,
                "hl": self.lang,
                "num": 10,
                "page": page,
                "autocorrect": self.autocorrect
            }
            headers = {
                "X-API-KEY": self.api_key,
                "Content-Type": "application/json"
            }

            for attempt in range(self.max_retries):
                try:
                    async with session.post(url, json=payload, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            results = data.get("organic", [])

                            for result in results:
                                original_position = result.get("position", 0)
                                result["actual_rank"] = (page - 1) * 10 + original_position
                                result["page"] = page

                            self.success_count += 1
                            self.log(f"âœ… {keyword} (é {page}): å–å¾— {len(results)} å€‹çµæœ")

                            return {
                                "keyword": keyword,
                                "page": page,
                                "results": results,
                                "success": True
                            }

                        elif response.status == 429:
                            wait_time = (attempt + 1) * 2
                            self.log(f"âš ï¸ {keyword} (é {page}): é™æµï¼Œç­‰å¾… {wait_time}s")
                            await asyncio.sleep(wait_time)
                            continue

                        else:
                            self.log(f"âŒ {keyword} (é {page}): HTTP {response.status}")
                            if attempt < self.max_retries - 1:
                                await asyncio.sleep(1)
                                continue

                except asyncio.TimeoutError:
                    self.log(f"â±ï¸ {keyword} (é {page}): è¶…æ™‚")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(1)
                        continue

                except Exception as e:
                    self.log(f"âŒ {keyword} (é {page}): {str(e)[:50]}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(1)
                        continue

            self.fail_count += 1
            return {"keyword": keyword, "page": page, "results": [], "success": False}

    async def search_all_async(self, keywords, max_pages, progress_callback=None):
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

        tasks_info = [(kw, page) for kw in keywords for page in range(1, max_pages + 1)]
        total_tasks = len(tasks_info)

        self.log(f"ğŸš€ é–‹å§‹: {len(keywords)} é—œéµå­— Ã— {max_pages} é  = {total_tasks} è«‹æ±‚")
        self.log(f"ğŸ“ Autocorrect: {'é–‹å•Ÿ' if self.autocorrect else 'é—œé–‰'}")

        semaphore = asyncio.Semaphore(self.max_concurrent)
        connector = aiohttp.TCPConnector(limit=self.max_concurrent, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)

        all_results = {kw: [] for kw in keywords}
        completed = 0

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            coroutines = [
                self._fetch_with_retry(session, kw, page, semaphore)
                for kw, page in tasks_info
            ]

            for coro in asyncio.as_completed(coroutines):
                result = await coro
                completed += 1

                keyword = result["keyword"]
                if result["success"] and result["results"]:
                    all_results[keyword].extend(result["results"])

                if progress_callback:
                    progress_callback(completed, total_tasks, keyword)

        for keyword in all_results:
            all_results[keyword].sort(key=lambda x: x.get("actual_rank", 999))

        self.log(f"ğŸ“Š å®Œæˆ: æˆåŠŸ={self.success_count}, å¤±æ•—={self.fail_count}")
        return all_results

    def search_all(self, keywords, max_pages, progress_callback=None):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(
                self.search_all_async(keywords, max_pages, progress_callback)
            )
        finally:
            loop.close()


class SequentialSerpSearcher:
    """é †åºæœå°‹å™¨"""

    def __init__(self, api_key, region="hk", lang="zh-tw", delay=0.3, autocorrect=False):
        self.api_key = api_key
        self.region = region
        self.lang = lang
        self.delay = delay
        self.autocorrect = autocorrect
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(pool_connections=5, pool_maxsize=5, max_retries=3)
        self.session.mount('https://', adapter)

    def log(self, message):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.debug_logs.append(f"[{timestamp}] {message}")

    def _fetch_single(self, keyword, page):
        url = "https://google.serper.dev/search"
        payload = {
            "q": keyword,
            "gl": self.region,
            "hl": self.lang,
            "num": 10,
            "page": page,
            "autocorrect": self.autocorrect
        }
        headers = {"X-API-KEY": self.api_key, "Content-Type": "application/json"}

        try:
            response = self.session.post(url, json=payload, headers=headers, timeout=15)

            if response.status_code == 200:
                data = response.json()
                results = data.get("organic", [])

                for result in results:
                    result["actual_rank"] = (page - 1) * 10 + result.get("position", 0)
                    result["page"] = page

                self.success_count += 1
                self.log(f"âœ… {keyword} (é {page}): {len(results)} çµæœ")
                return results

            elif response.status_code == 429:
                self.log(f"âš ï¸ {keyword} (é {page}): é™æµ")
                time.sleep(2)
                return self._fetch_single(keyword, page)
            else:
                self.log(f"âŒ {keyword} (é {page}): HTTP {response.status_code}")
                self.fail_count += 1

        except Exception as e:
            self.log(f"âŒ {keyword} (é {page}): {str(e)[:30]}")
            self.fail_count += 1

        return []

    def search_all(self, keywords, max_pages, progress_callback=None):
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

        total = len(keywords) * max_pages
        completed = 0
        all_results = {}

        for keyword in keywords:
            all_results[keyword] = []
            for page in range(1, max_pages + 1):
                results = self._fetch_single(keyword, page)
                all_results[keyword].extend(results)
                completed += 1
                if progress_callback:
                    progress_callback(completed, total, keyword)
                time.sleep(self.delay)
            all_results[keyword].sort(key=lambda x: x.get("actual_rank", 999))

        return all_results


class BatchSerpSearcher:
    """æ‰¹æ¬¡æœå°‹å™¨"""

    def __init__(self, api_key, region="hk", lang="zh-tw",
                 batch_size=5, delay_between_batches=1.0, max_workers=5, autocorrect=False):
        self.api_key = api_key
        self.region = region
        self.lang = lang
        self.batch_size = batch_size
        self.batch_delay = delay_between_batches
        self.max_workers = max_workers
        self.autocorrect = autocorrect
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(pool_connections=max_workers, pool_maxsize=max_workers, max_retries=3)
        self.session.mount('https://', adapter)

    def log(self, message):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.debug_logs.append(f"[{timestamp}] {message}")

    def _fetch_single(self, keyword, page):
        url = "https://google.serper.dev/search"
        payload = {
            "q": keyword,
            "gl": self.region,
            "hl": self.lang,
            "num": 10,
            "page": page,
            "autocorrect": self.autocorrect
        }
        headers = {"X-API-KEY": self.api_key, "Content-Type": "application/json"}

        for attempt in range(3):
            try:
                response = self.session.post(url, json=payload, headers=headers, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    results = data.get("organic", [])
                    for result in results:
                        result["actual_rank"] = (page - 1) * 10 + result.get("position", 0)
                        result["page"] = page
                    self.success_count += 1
                    return {"keyword": keyword, "page": page, "results": results, "success": True}
                elif response.status_code == 429:
                    time.sleep((attempt + 1) * 2)
                    continue
            except Exception:
                if attempt < 2:
                    time.sleep(1)
                    continue

        self.fail_count += 1
        return {"keyword": keyword, "page": page, "results": [], "success": False}

    def search_all(self, keywords, max_pages, progress_callback=None):
        self.debug_logs = []
        self.success_count = 0
        self.fail_count = 0

        all_tasks = [(kw, page) for kw in keywords for page in range(1, max_pages + 1)]
        total_tasks = len(all_tasks)
        batches = [all_tasks[i:i + self.batch_size] for i in range(0, len(all_tasks), self.batch_size)]

        all_results = {kw: [] for kw in keywords}
        completed = 0

        for batch_idx, batch in enumerate(batches):
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self._fetch_single, kw, page): (kw, page) for kw, page in batch}
                for future in as_completed(futures):
                    result = future.result()
                    completed += 1
                    if result["success"]:
                        all_results[result["keyword"]].extend(result["results"])
                    if progress_callback:
                        progress_callback(completed, total_tasks, result["keyword"])

            if batch_idx < len(batches) - 1:
                time.sleep(self.batch_delay)

        for keyword in all_results:
            all_results[keyword].sort(key=lambda x: x.get("actual_rank", 999))

        return all_results


# ============ æ’ååˆ†æå·¥å…· ============

def find_rankings(serp_results, sites):
    """å¾ SERP çµæœä¸­æ‰¾å‡ºæŒ‡å®šç¶²ç«™çš„æ’å"""
    rankings = []

    for keyword, results in serp_results.items():
        row = {"keyword": keyword}

        for site in sites:
            rank = None
            site_normalized = normalize_domain(site)
            for result in results:
                link = result.get("link", "")
                link_normalized = normalize_domain(link)
                if site_normalized in link_normalized:
                    rank = result.get("actual_rank")
                    break
            row[site] = rank

        rankings.append(row)

    return rankings


def analyze_site_rankings(rankings, site, warning_threshold=20):
    """åˆ†æå–®ä¸€ç¶²ç«™çš„æ’ååˆ†ä½ˆ"""
    analysis = {
        "top3": [],
        "top10": [],
        "top20": [],
        "top30": [],
        "warning": [],
        "na": []
    }

    for item in rankings:
        keyword = item.get("keyword")
        rank = item.get(site)

        if rank is None:
            analysis["na"].append(keyword)
        elif rank <= 3:
            analysis["top3"].append({"keyword": keyword, "rank": rank})
        elif rank <= 10:
            analysis["top10"].append({"keyword": keyword, "rank": rank})
        elif rank <= 20:
            analysis["top20"].append({"keyword": keyword, "rank": rank})
        elif rank <= 30:
            analysis["top30"].append({"keyword": keyword, "rank": rank})

        if rank is not None and rank > warning_threshold:
            analysis["warning"].append({"keyword": keyword, "rank": rank})

    return analysis


def create_styled_ranking_dataframe(rankings, my_sites, competitors, warning_threshold, previous_rankings=None):
    """å‰µå»ºå¸¶æ¨£å¼çš„æ’å DataFrame"""
    all_sites = my_sites + competitors

    # å»ºç«‹é¡¯ç¤ºæ•¸æ“š
    display_data = []
    for rank_data in rankings:
        row = {"é—œéµå­—": rank_data.get("keyword")}

        for site in all_sites:
            rank = rank_data.get(site)
            kw = rank_data.get("keyword")

            # è¨ˆç®—è®ŠåŒ–
            change = ""
            if previous_rankings and kw in previous_rankings:
                prev_rank = previous_rankings[kw].get(site)
                if prev_rank is not None and rank is not None:
                    diff = prev_rank - rank
                    if diff > 0:
                        change = f" â†‘{diff}"
                    elif diff < 0:
                        change = f" â†“{abs(diff)}"
                    else:
                        change = " â”€"

            row[site] = f"{rank}{change}" if rank is not None else "N/A"

        display_data.append(row)

    df_display = pd.DataFrame(display_data)

    # æ¨£å¼å‡½æ•¸
    def style_ranking_cell(val, col_name):
        is_my_site = col_name in my_sites

        if "N/A" in str(val):
            if is_my_site:
                return "background-color: #FEF2F2; color: #B91C1C;"
            else:
                return "background-color: #F9FAFB; color: #9CA3AF;"

        try:
            rank = int(str(val).split()[0])

            if is_my_site:
                if rank <= 3:
                    return "background-color: #DBEAFE; color: #1E40AF; font-weight: bold;"
                elif rank <= 10:
                    return "background-color: #E0F2FE; color: #0369A1;"
                elif rank <= 20:
                    return "background-color: #F0F9FF; color: #0C4A6E;"
                elif rank > warning_threshold:
                    return "background-color: #FEE2E2; color: #DC2626; font-weight: bold;"
                else:
                    return "background-color: #F8FAFC; color: #475569;"
            else:
                if rank <= 3:
                    return "background-color: #FEF3C7; color: #92400E; font-weight: bold;"
                elif rank <= 10:
                    return "background-color: #FFFBEB; color: #B45309;"
                elif rank <= 20:
                    return "background-color: #F9FAFB; color: #6B7280;"
                else:
                    return "background-color: #F3F4F6; color: #9CA3AF;"
        except:
            return ""

    def apply_styles(df):
        styles = pd.DataFrame('', index=df.index, columns=df.columns)
        for col in df.columns:
            if col != "é—œéµå­—":
                styles[col] = df[col].apply(lambda x: style_ranking_cell(x, col))
        return styles

    styled_df = df_display.style.apply(lambda _: apply_styles(df_display), axis=None)

    return df_display, styled_df


# ============ åˆå§‹åŒ– Session State ============

if "history" not in st.session_state:
    st.session_state.history = load_history()

if "keyword_groups" not in st.session_state:
    st.session_state.keyword_groups = load_keyword_groups()

if "current_results" not in st.session_state:
    st.session_state.current_results = None

if "debug_logs" not in st.session_state:
    st.session_state.debug_logs = []

if "keywords_input" not in st.session_state:
    st.session_state.keywords_input = "åˆ°æœƒ\nåˆ°æœƒæ¨ä»‹\næ´¾å°åˆ°æœƒ"

if "current_tab" not in st.session_state:
    st.session_state.current_tab = 0

# ============ æ¨™é¡Œ ============

st.markdown("""
<div class="main-header">
    <h1>ğŸš€ SEO æ’åè¿½è¹¤å·¥å…· Pro</h1>
    <p>ç©©å®šé«˜é€Ÿç‰ˆ Â· æ™ºèƒ½åˆ†æ Â· ç«¶çˆ­å°æ‰‹è¿½è¹¤</p>
</div>
""", unsafe_allow_html=True)

# ============ å´é‚Šæ¬„è¨­å®š ============

with st.sidebar:
    st.markdown("## âš™ï¸ è¨­å®š")

    api_key = st.text_input("ğŸ”‘ Serper API Key", type="password")

    if api_key:
        st.success("âœ… API Key å·²è¨­å®š")
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥ API Key")

    st.markdown("---")

    st.markdown("### ğŸ” æœå°‹è¨­å®š")

    col1, col2 = st.columns(2)
    with col1:
        search_region = st.selectbox(
            "åœ°å€",
            options=["hk", "tw", "sg", "my", "us", "uk"],
            format_func=lambda x: {"hk": "ğŸ‡­ğŸ‡° é¦™æ¸¯", "tw": "ğŸ‡¹ğŸ‡¼ å°ç£", "sg": "ğŸ‡¸ğŸ‡¬ æ–°åŠ å¡",
                                   "my": "ğŸ‡²ğŸ‡¾ é¦¬ä¾†è¥¿äº", "us": "ğŸ‡ºğŸ‡¸ ç¾åœ‹", "uk": "ğŸ‡¬ğŸ‡§ è‹±åœ‹"}[x]
        )

    with col2:
        search_lang = st.selectbox(
            "èªè¨€",
            options=["zh-tw", "zh-cn", "en"],
            format_func=lambda x: {"zh-tw": "ç¹é«”ä¸­æ–‡", "zh-cn": "ç®€ä½“ä¸­æ–‡", "en": "English"}[x]
        )

    max_pages = st.slider("ğŸ“„ çˆ¬å–é æ•¸", 1, 10, 5)

    # æ–°å¢ Autocorrect é–‹é—œ
    autocorrect_enabled = st.toggle(
        "ğŸ”¤ è‡ªå‹•æ ¡æ­£ (Autocorrect)",
        value=False,
        help="é—œé–‰æ™‚æœƒæœå°‹åŸå§‹é—œéµå­—ï¼Œé–‹å•Ÿæ™‚ Google æœƒè‡ªå‹•æ ¡æ­£æ‹¼å¯«éŒ¯èª¤"
    )

    if not autocorrect_enabled:
        st.caption("ğŸ“ å·²é—œé–‰è‡ªå‹•æ ¡æ­£ï¼Œå°‡æœå°‹åŸå§‹é—œéµå­—")
    else:
        st.caption("ğŸ“ å·²é–‹å•Ÿè‡ªå‹•æ ¡æ­£ï¼ŒGoogle å¯èƒ½ä¿®æ”¹æœå°‹è©")

    st.markdown("---")

    st.markdown("### âš¡ é€Ÿåº¦æ¨¡å¼")
    speed_mode = st.radio(
        "é¸æ“‡æ¨¡å¼",
        options=["stable", "balanced", "fast"],
        format_func=lambda x: {
            "stable": "ğŸ¢ ç©©å®šæ¨¡å¼",
            "balanced": "âš–ï¸ å¹³è¡¡æ¨¡å¼ (æ¨è–¦)",
            "fast": "ğŸš€ é«˜é€Ÿæ¨¡å¼"
        }[x],
        index=1
    )

    if speed_mode == "stable":
        max_concurrent = 1
        delay = 0.5
    elif speed_mode == "balanced":
        max_concurrent = 5
        delay = 0.3
    else:
        max_concurrent = st.slider("æœ€å¤§ä¸¦ç™¼æ•¸", 5, 30, 15)
        delay = 0.1

    st.markdown("---")

    # é è¨­ç¶²ç«™
    st.markdown("### ğŸ  æˆ‘çš„ç¶²ç«™")
    default_my_sites = """daynightcatering.com
cateringbear.com
ceocatering.com
cateringmoment.com"""

    my_sites_input = st.text_area(
        "æ¯è¡Œä¸€å€‹ç¶²åŸŸ",
        value=default_my_sites,
        height=100,
        key="my_sites"
    )
    my_sites = [s.strip() for s in my_sites_input.split("\n") if s.strip()]

    st.markdown("### ğŸ¯ ç«¶çˆ­å°æ‰‹")
    competitors_input = st.text_area(
        "æ¯è¡Œä¸€å€‹ç¶²åŸŸ",
        value="cateringmama.com\nkamadelivery.com",
        height=80,
        key="competitors"
    )
    competitors = [s.strip() for s in competitors_input.split("\n") if s.strip()]

    st.markdown("---")

    st.markdown("### ğŸ¨ é¡¯ç¤ºè¨­å®š")
    warning_threshold = st.number_input(
        "âš ï¸ è­¦å‘Šé–¾å€¼ï¼ˆæ’åè¶…éæ­¤æ•¸å­—æ¨™ç´…ï¼‰",
        min_value=10,
        max_value=100,
        value=20,
        step=5,
        help="æ’åè¶…éé€™å€‹æ•¸å­—çš„æœƒç”¨ç´…è‰²æ¨™ç¤º"
    )

    st.markdown("---")
    debug_mode = st.checkbox("ğŸ› é¡¯ç¤ºèª¿è©¦ä¿¡æ¯", value=False)

# ============ å›ºå®šå°èˆªæŒ‰éˆ• ============

st.markdown("---")

# ä½¿ç”¨ columns å‰µå»ºå›ºå®šå°èˆª - èª¿æ•´é †åºï¼šæ•¸æ“šåˆ†æåœ¨æ­·å²è¨˜éŒ„å‰é¢
nav_cols = st.columns(5)

tab_names = ["ğŸ” æ’åæŸ¥è©¢", "ğŸ·ï¸ é—œéµå­—ç®¡ç†", "ğŸ“Š æ•¸æ“šåˆ†æ", "ğŸ“ˆ æ­·å²è¨˜éŒ„", "âš™ï¸ ç®¡ç†"]

for i, (col, name) in enumerate(zip(nav_cols, tab_names)):
    with col:
        if st.button(name, key=f"nav_{i}", use_container_width=True,
                     type="primary" if st.session_state.current_tab == i else "secondary"):
            st.session_state.current_tab = i
            st.rerun()

st.markdown("---")

# ============ Tab å…§å®¹ ============

# ============ Tab 0: æ’åæŸ¥è©¢ ============

if st.session_state.current_tab == 0:
    col_left, col_right = st.columns([2, 1])

    with col_left:
        st.markdown("### ğŸ“ è¼¸å…¥é—œéµå­—")
        keywords_input = st.text_area(
            "æ¯è¡Œä¸€å€‹é—œéµå­—",
            value=st.session_state.keywords_input,
            height=200,
            key="keywords_text_area"
        )
        # åŒæ­¥åˆ° session state
        st.session_state.keywords_input = keywords_input
        keywords = [k.strip() for k in keywords_input.split("\n") if k.strip()]

    with col_right:
        st.markdown("### ğŸ“‚ é—œéµå­—çµ„ï¼ˆé»æ“Šè¤‡è£½ï¼‰")

        keyword_groups = st.session_state.keyword_groups

        if keyword_groups:
            selected_group = st.selectbox(
                "é¸æ“‡é—œéµå­—çµ„æŸ¥çœ‹",
                options=["é¸æ“‡..."] + list(keyword_groups.keys()),
                key="view_group_select"
            )

            if selected_group != "é¸æ“‡...":
                group_data = keyword_groups[selected_group]
                group_keywords = group_data.get("keywords", [])
                group_desc = group_data.get("description", "")

                st.markdown(f"**{selected_group}** ({len(group_keywords)}å€‹é—œéµå­—)")
                if group_desc:
                    st.caption(f"ğŸ“ {group_desc}")

                # é¡¯ç¤ºé—œéµå­—å…§å®¹ï¼Œå¯ä»¥è¤‡è£½
                keywords_text = "\n".join(group_keywords)
                st.code(keywords_text, language=None)

                st.caption("ğŸ‘† é»æ“Šå³ä¸Šè§’è¤‡è£½æŒ‰éˆ•ï¼Œç„¶å¾Œè²¼åˆ°å·¦é‚Šè¼¸å…¥æ¡†")
        else:
            st.info("ğŸ’¡ é‚„æ²’æœ‰é—œéµå­—çµ„ï¼Œè«‹åˆ°ã€Œé—œéµå­—ç®¡ç†ã€å»ºç«‹")

        st.markdown("---")
        st.markdown("### ğŸ“‹ æŸ¥è©¢è³‡è¨Š")
        st.markdown(f"**é—œéµå­—æ•¸é‡ï¼š** {len(keywords)}")
        st.markdown(f"**API è«‹æ±‚æ•¸ï¼š** {len(keywords) * max_pages}")
        st.markdown(f"**è­¦å‘Šé–¾å€¼ï¼š** æ’å > {warning_threshold}")
        st.markdown(f"**è‡ªå‹•æ ¡æ­£ï¼š** {'é–‹å•Ÿ' if autocorrect_enabled else 'é—œé–‰'}")

    st.markdown("---")

    col_btn1, col_btn2, col_btn3 = st.columns([1, 2, 1])
    with col_btn2:
        start_tracking = st.button("ğŸš€ é–‹å§‹è¿½è¹¤æ’å", type="primary", use_container_width=True)

    # ============ åŸ·è¡Œæœå°‹ ============

    if start_tracking:
        if not api_key:
            st.error("âŒ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ API Key")
            st.stop()

        if not keywords:
            st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
            st.stop()

        all_sites = my_sites + competitors
        if not all_sites:
            st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹è¦è¿½è¹¤çš„ç¶²ç«™")
            st.stop()

        progress_container = st.container()
        with progress_container:
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                progress_bar = st.progress(0)
                status_text = st.empty()
            with col2:
                time_display = st.empty()
            with col3:
                stats_display = st.empty()

        start_time = time.time()


        def update_progress(completed, total, current_keyword):
            progress = completed / total
            progress_bar.progress(progress)
            elapsed = time.time() - start_time
            status_text.text(f"è™•ç†: {current_keyword}")
            time_display.markdown(f"**â±ï¸ {elapsed:.1f}s**")
            stats_display.markdown(f"**{completed}/{total}**")


        if speed_mode == "stable":
            searcher = SequentialSerpSearcher(api_key, search_region, search_lang, delay, autocorrect_enabled)
        elif speed_mode == "balanced":
            searcher = BatchSerpSearcher(api_key, search_region, search_lang, max_concurrent, 0.5, max_concurrent,
                                         autocorrect_enabled)
        else:
            searcher = StableSerpSearcher(api_key, search_region, search_lang, max_concurrent, delay, 3,
                                          autocorrect_enabled)

        serp_results = searcher.search_all(keywords, max_pages, update_progress)
        elapsed_time = time.time() - start_time

        st.session_state.debug_logs = searcher.debug_logs

        if debug_mode and searcher.debug_logs:
            with st.expander("ğŸ› èª¿è©¦æ—¥èªŒ", expanded=True):
                log_text = "\n".join(searcher.debug_logs[-50:])
                st.markdown(f'<div class="debug-box">{log_text}</div>', unsafe_allow_html=True)

        all_rankings = find_rankings(serp_results, all_sites)
        progress_bar.progress(1.0)

        success_rate = searcher.success_count / (searcher.success_count + searcher.fail_count) * 100 if (
                                                                                                                searcher.success_count + searcher.fail_count) > 0 else 0

        if success_rate >= 90:
            st.success(f"âœ… å®Œæˆï¼è€—æ™‚ {elapsed_time:.1f}sï¼ŒæˆåŠŸç‡ {success_rate:.0f}%")
        elif success_rate >= 70:
            st.warning(f"âš ï¸ å®Œæˆï¼Œéƒ¨åˆ†å¤±æ•—ã€‚è€—æ™‚ {elapsed_time:.1f}sï¼ŒæˆåŠŸç‡ {success_rate:.0f}%")
        else:
            st.error(f"âŒ å¤§é‡å¤±æ•—ã€‚æˆåŠŸç‡ {success_rate:.0f}%ï¼Œå»ºè­°åˆ‡æ›åˆ°ç©©å®šæ¨¡å¼")

        st.session_state.current_results = {
            "rankings": all_rankings,
            "serp_data": serp_results,
            "timestamp": datetime.now().isoformat(),
            "elapsed_time": elapsed_time,
            "success_rate": success_rate,
            "my_sites": my_sites,
            "competitors": competitors
        }

        record = {
            "rankings": all_rankings,
            "my_sites": my_sites,
            "competitors": competitors,
            "region": search_region,
            "keywords": keywords,
            "autocorrect": autocorrect_enabled
        }
        st.session_state.history = add_record(st.session_state.history, record)

    # ============ é¡¯ç¤ºçµæœ ============

    if st.session_state.current_results:
        st.markdown("---")

        results = st.session_state.current_results
        rankings = results["rankings"]
        result_my_sites = results.get("my_sites", my_sites)
        result_competitors = results.get("competitors", competitors)

        # ç²å–ä¸Šæ¬¡è¨˜éŒ„ç”¨æ–¼æ¯”è¼ƒ
        history_records = st.session_state.history.get("records", [])
        previous_rankings = {}
        if len(history_records) >= 2:
            prev_record = history_records[-2]
            for item in prev_record.get("rankings", []):
                previous_rankings[item.get("keyword")] = item

        # ============ è©³ç´°æ’åè¡¨æ ¼ï¼ˆç§»åˆ°æœ€ä¸Šæ–¹ï¼‰ ============

        st.markdown("## ğŸ“‹ è©³ç´°æ’å")

        st.markdown("""
        **åœ–ä¾‹ï¼š** ğŸ”µ æˆ‘çš„ç¶²ç«™ï¼ˆè—è‰²ç³»ï¼‰| ğŸŸ  ç«¶çˆ­å°æ‰‹ï¼ˆæ©™è‰²ç³»ï¼‰| âš ï¸ ç´…è‰² = æ’å > {} | N/A = æœªä¸Šæ¦œ
        """.format(warning_threshold))

        df_display, styled_df = create_styled_ranking_dataframe(
            rankings, result_my_sites, result_competitors, warning_threshold, previous_rankings
        )

        st.dataframe(styled_df, use_container_width=True, height=500)

        # ä¸‹è¼‰æŒ‰éˆ•
        def create_excel(rankings, serp_data, my_sites_list, competitors_list):
            output = BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                df_rankings = pd.DataFrame(rankings)
                df_rankings.to_excel(writer, sheet_name="æ’åç¸½è¦½", index=False)

                serp_records = []
                for keyword, results in serp_data.items():
                    for result in results:
                        serp_records.append({
                            "é—œéµå­—": keyword,
                            "æ’å": result.get("actual_rank"),
                            "æ¨™é¡Œ": result.get("title"),
                            "ç¶²å€": result.get("link"),
                            "æè¿°": result.get("snippet", "")[:200]
                        })
                if serp_records:
                    pd.DataFrame(serp_records).to_excel(writer, sheet_name="å®Œæ•´SERP", index=False)

            output.seek(0)
            return output


        col_dl1, col_dl2 = st.columns(2)

        with col_dl1:
            excel_file = create_excel(rankings, results.get("serp_data", {}), result_my_sites, result_competitors)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š",
                data=excel_file,
                file_name=f"serp_ranking_{timestamp}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

        with col_dl2:
            csv_data = df_display.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ CSV",
                data=csv_data,
                file_name=f"serp_ranking_{timestamp}.csv",
                mime="text/csv",
                use_container_width=True
            )

        # ============ æ’åç¸½è¦½ï¼ˆç§»åˆ°è¡¨æ ¼ä¸‹æ–¹ï¼‰ ============

        st.markdown("---")
        st.markdown("## ğŸ“Š æ’åç¸½è¦½")

        # æˆ‘çš„ç¶²ç«™
        if result_my_sites:
            st.markdown("### ğŸ  æˆ‘çš„ç¶²ç«™")

            for site in result_my_sites:
                analysis = analyze_site_rankings(rankings, site, warning_threshold)

                with st.expander(f"ğŸ“Š **{site}**", expanded=True):
                    cols = st.columns(6)

                    categories = [
                        ("ğŸ† å‰3å", "top3", "#10B981"),
                        ("ğŸ“„ é¦–é (4-10)", "top10", "#3B82F6"),
                        ("ğŸ“‘ ç¬¬2é (11-20)", "top20", "#F59E0B"),
                        ("ğŸ“‹ ç¬¬3é (21-30)", "top30", "#8B5CF6"),
                        (f"âš ï¸ >{warning_threshold}å", "warning", "#EF4444"),
                        ("âŒ æœªä¸Šæ¦œ", "na", "#6B7280")
                    ]

                    for i, (label, key, color) in enumerate(categories):
                        with cols[i]:
                            count = len(analysis[key])
                            st.markdown(f"""
                            <div style="text-align: center; padding: 0.5rem; background: white; border-radius: 8px; border-left: 3px solid {color};">
                                <div style="font-size: 1.5rem; font-weight: bold; color: {color};">{count}</div>
                                <div style="font-size: 0.75rem; color: #666;">{label}</div>
                            </div>
                            """, unsafe_allow_html=True)

                    st.markdown("---")

                    tab_top3, tab_top10, tab_top20, tab_top30, tab_warning, tab_na = st.tabs([
                        f"ğŸ† å‰3å ({len(analysis['top3'])})",
                        f"ğŸ“„ é¦–é  ({len(analysis['top10'])})",
                        f"ğŸ“‘ ç¬¬2é  ({len(analysis['top20'])})",
                        f"ğŸ“‹ ç¬¬3é  ({len(analysis['top30'])})",
                        f"âš ï¸ è­¦å‘Š ({len(analysis['warning'])})",
                        f"âŒ æœªä¸Šæ¦œ ({len(analysis['na'])})"
                    ])

                    with tab_top3:
                        if analysis["top3"]:
                            for item in sorted(analysis["top3"], key=lambda x: x["rank"]):
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨å‰3å")

                    with tab_top10:
                        if analysis["top10"]:
                            for item in sorted(analysis["top10"], key=lambda x: x["rank"]):
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨4-10å")

                    with tab_top20:
                        if analysis["top20"]:
                            for item in sorted(analysis["top20"], key=lambda x: x["rank"]):
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨11-20å")

                    with tab_top30:
                        if analysis["top30"]:
                            for item in sorted(analysis["top30"], key=lambda x: x["rank"]):
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨21-30å")

                    with tab_warning:
                        if analysis["warning"]:
                            st.warning(f"ä»¥ä¸‹ {len(analysis['warning'])} å€‹é—œéµå­—æ’åè¶…é {warning_threshold}ï¼š")
                            for item in sorted(analysis["warning"], key=lambda x: x["rank"]):
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.success(f"æ‰€æœ‰é—œéµå­—éƒ½åœ¨ {warning_threshold} åå…§ï¼")

                    with tab_na:
                        if analysis["na"]:
                            st.error(f"ä»¥ä¸‹ {len(analysis['na'])} å€‹é—œéµå­—æœªä¸Šæ¦œï¼š")
                            for kw in analysis["na"]:
                                st.markdown(f"â€¢ {kw}")
                        else:
                            st.success("æ‰€æœ‰é—œéµå­—éƒ½æœ‰æ’åï¼")

        # ç«¶çˆ­å°æ‰‹
        if result_competitors:
            st.markdown("---")
            st.markdown("### ğŸ¯ ç«¶çˆ­å°æ‰‹")

            for site in result_competitors:
                analysis = analyze_site_rankings(rankings, site, warning_threshold)

                with st.expander(f"ğŸ“Š **{site}**", expanded=False):
                    cols = st.columns(6)

                    categories = [
                        ("ğŸ† å‰3å", "top3", "#F59E0B"),
                        ("ğŸ“„ é¦–é (4-10)", "top10", "#D97706"),
                        ("ğŸ“‘ ç¬¬2é (11-20)", "top20", "#92400E"),
                        ("ğŸ“‹ ç¬¬3é (21-30)", "top30", "#78350F"),
                        (f">{warning_threshold}å", "warning", "#9CA3AF"),
                        ("âŒ æœªä¸Šæ¦œ", "na", "#D1D5DB")
                    ]

                    for i, (label, key, color) in enumerate(categories):
                        with cols[i]:
                            count = len(analysis[key])
                            st.markdown(f"""
                            <div style="text-align: center; padding: 0.5rem; background: white; border-radius: 8px; border-left: 3px solid {color};">
                                <div style="font-size: 1.5rem; font-weight: bold; color: {color};">{count}</div>
                                <div style="font-size: 0.75rem; color: #666;">{label}</div>
                            </div>
                            """, unsafe_allow_html=True)

                    if analysis["top3"] or analysis["top10"]:
                        st.markdown("**å„ªå‹¢é—œéµå­—ï¼š**")
                        for item in sorted(analysis["top3"] + analysis["top10"], key=lambda x: x["rank"]):
                            st.markdown(f"**#{item['rank']}** - {item['keyword']}")

# ============ Tab 1: é—œéµå­—ç®¡ç† ============

elif st.session_state.current_tab == 1:
    st.markdown("### ğŸ·ï¸ é—œéµå­—çµ„ç®¡ç†")

    col_left, col_right = st.columns([1, 1])

    with col_left:
        st.markdown("#### â• æ–°å¢é—œéµå­—çµ„")

        new_group_name = st.text_input("çµ„å", placeholder="ä¾‹å¦‚ï¼šåˆ°æœƒç›¸é—œ", key="new_group_name")
        new_group_keywords = st.text_area(
            "é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰",
            height=200,
            placeholder="åˆ°æœƒ\nåˆ°æœƒæ¨ä»‹\næ´¾å°åˆ°æœƒ\n...",
            key="new_group_keywords"
        )
        new_group_desc = st.text_input("æè¿°ï¼ˆé¸å¡«ï¼‰", placeholder="ä¾‹å¦‚ï¼šåˆ°æœƒæœå‹™ç›¸é—œé—œéµå­—", key="new_group_desc")

        if st.button("ğŸ’¾ å„²å­˜é—œéµå­—çµ„", type="primary", use_container_width=True):
            if not new_group_name:
                st.error("âŒ è«‹è¼¸å…¥çµ„å")
            elif not new_group_keywords.strip():
                st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
            else:
                keywords_list = [k.strip() for k in new_group_keywords.split("\n") if k.strip()]
                st.session_state.keyword_groups[new_group_name] = {
                    "keywords": keywords_list,
                    "description": new_group_desc,
                    "created": datetime.now().isoformat(),
                    "updated": datetime.now().isoformat()
                }
                save_keyword_groups(st.session_state.keyword_groups)
                st.success(f"âœ… å·²å„²å­˜ã€Œ{new_group_name}ã€ï¼ˆ{len(keywords_list)} å€‹é—œéµå­—ï¼‰")
                st.rerun()

    with col_right:
        st.markdown("#### ğŸ“‹ ç¾æœ‰é—œéµå­—çµ„")

        keyword_groups = st.session_state.keyword_groups

        if not keyword_groups:
            st.info("ğŸ’¡ é‚„æ²’æœ‰é—œéµå­—çµ„ï¼Œè«‹åœ¨å·¦å´æ–°å¢")
        else:
            for group_name, group_data in keyword_groups.items():
                group_keywords = group_data.get("keywords", [])
                group_desc = group_data.get("description", "")

                with st.expander(f"ğŸ“ {group_name} ({len(group_keywords)}å€‹)", expanded=False):
                    st.markdown(f"**æè¿°ï¼š** {group_desc if group_desc else 'ç„¡'}")

                    # é¡¯ç¤ºé—œéµå­—å…§å®¹ï¼Œå¯ä»¥è¤‡è£½
                    st.markdown("**é—œéµå­—ï¼š**ï¼ˆé»æ“Šå³ä¸Šè§’è¤‡è£½ï¼‰")
                    keywords_text = "\n".join(group_keywords)
                    st.code(keywords_text, language=None)

                    col1, col2 = st.columns(2)

                    with col1:
                        if st.button("âœï¸ ç·¨è¼¯", key=f"edit_{group_name}", use_container_width=True):
                            st.session_state[f"editing_{group_name}"] = True
                            st.rerun()

                    with col2:
                        if st.button("ğŸ—‘ï¸ åˆªé™¤", key=f"delete_{group_name}", use_container_width=True):
                            del st.session_state.keyword_groups[group_name]
                            save_keyword_groups(st.session_state.keyword_groups)
                            st.success(f"âœ… å·²åˆªé™¤ã€Œ{group_name}ã€")
                            st.rerun()

                    # ç·¨è¼¯æ¨¡å¼
                    if st.session_state.get(f"editing_{group_name}", False):
                        st.markdown("---")
                        st.markdown("**âœï¸ ç·¨è¼¯æ¨¡å¼**")

                        edit_keywords = st.text_area(
                            "ä¿®æ”¹é—œéµå­—",
                            value="\n".join(group_keywords),
                            height=150,
                            key=f"edit_kw_{group_name}"
                        )
                        edit_desc = st.text_input(
                            "ä¿®æ”¹æè¿°",
                            value=group_desc,
                            key=f"edit_desc_{group_name}"
                        )

                        col_save, col_cancel = st.columns(2)
                        with col_save:
                            if st.button("ğŸ’¾ å„²å­˜", key=f"save_{group_name}", type="primary", use_container_width=True):
                                new_keywords = [k.strip() for k in edit_keywords.split("\n") if k.strip()]
                                st.session_state.keyword_groups[group_name]["keywords"] = new_keywords
                                st.session_state.keyword_groups[group_name]["description"] = edit_desc
                                st.session_state.keyword_groups[group_name]["updated"] = datetime.now().isoformat()
                                save_keyword_groups(st.session_state.keyword_groups)
                                st.session_state[f"editing_{group_name}"] = False
                                st.success("âœ… å·²æ›´æ–°")
                                st.rerun()

                        with col_cancel:
                            if st.button("âŒ å–æ¶ˆ", key=f"cancel_{group_name}", use_container_width=True):
                                st.session_state[f"editing_{group_name}"] = False
                                st.rerun()

    st.markdown("---")

    # åŒ¯å…¥/åŒ¯å‡ºåŠŸèƒ½
    st.markdown("#### ğŸ“¤ åŒ¯å…¥/åŒ¯å‡º")

    col1, col2 = st.columns(2)

    with col1:
        if keyword_groups:
            json_data = json.dumps(keyword_groups, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ åŒ¯å‡ºæ‰€æœ‰é—œéµå­—çµ„",
                data=json_data,
                file_name=f"keyword_groups_{datetime.now().strftime('%Y%m%d')}.json",
                mime="application/json",
                use_container_width=True
            )

    with col2:
        uploaded_file = st.file_uploader("ä¸Šå‚³é—œéµå­—çµ„ JSON", type=["json"], key="upload_kw_groups")
        if uploaded_file:
            try:
                imported_groups = json.load(uploaded_file)
                if st.button("ç¢ºèªåŒ¯å…¥", use_container_width=True):
                    st.session_state.keyword_groups.update(imported_groups)
                    save_keyword_groups(st.session_state.keyword_groups)
                    st.success(f"âœ… å·²åŒ¯å…¥ {len(imported_groups)} å€‹é—œéµå­—çµ„")
                    st.rerun()
            except Exception as e:
                st.error(f"åŒ¯å…¥å¤±æ•—ï¼š{e}")

# ============ Tab 2: æ•¸æ“šåˆ†æï¼ˆç§»åˆ°æ­·å²è¨˜éŒ„å‰é¢ï¼‰============

elif st.session_state.current_tab == 2:
    st.markdown("### ğŸ“Š SEO æ•¸æ“šåˆ†æ")

    history_records = st.session_state.history.get("records", [])

    if not history_records:
        st.info("ğŸ“Š é‚„æ²’æœ‰æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œæ’åæŸ¥è©¢")
    else:
        # åˆ†æé é¢çš„è­¦å‘Šé–¾å€¼
        analysis_warning_threshold = st.number_input(
            "âš ï¸ åˆ†æè­¦å‘Šé–¾å€¼",
            min_value=10,
            max_value=100,
            value=20,
            step=5,
            key="analysis_warning_threshold"
        )

        st.markdown("---")

        # ============ é¸æ“‡æ­·å²è¨˜éŒ„ ============
        
        st.markdown("### ğŸ“… é¸æ“‡æŸ¥è©¢è¨˜éŒ„")
        
        record_options = []
        for i, record in enumerate(reversed(history_records)):
            record_idx = len(history_records) - 1 - i
            display_name = get_record_display_name(record)
            record_options.append((display_name, record_idx))
        
        selected_record_display = st.selectbox(
            "é¸æ“‡è¦åˆ†æçš„è¨˜éŒ„",
            options=[opt[0] for opt in record_options],
            key="analysis_record_select"
        )
        
        # ç²å–é¸ä¸­çš„è¨˜éŒ„
        selected_record_idx = None
        for display_name, idx in record_options:
            if display_name == selected_record_display:
                selected_record_idx = idx
                break
        
        if selected_record_idx is not None:
            selected_record = history_records[selected_record_idx]
            rankings = selected_record.get("rankings", [])
            tracked_my_sites = selected_record.get("my_sites", [])
            tracked_competitors = selected_record.get("competitors", [])
            all_sites_in_record = get_all_sites_from_record(selected_record)
            keyword_order_map = get_keyword_order_map(selected_record)

            st.info(f"ğŸ“Š åˆ†æè¨˜éŒ„: {selected_record.get('date', '')} {selected_record.get('time', '')} | {len(rankings)} å€‹é—œéµå­— | {len(all_sites_in_record)} å€‹ç¶²ç«™")

            st.markdown("---")

            if rankings:
                # ============ 1. é—œéµå­—çˆ­å¥ªåˆ†æï¼ˆç¬¬ä¸€ä½ï¼‰============

                st.markdown("### ğŸ¥Š é—œéµå­—çˆ­å¥ªåˆ†æ")
                
                st.markdown("**å¯ä»¥æ¯”è¼ƒä»»æ„å…©å€‹ç¶²ç«™ä¹‹é–“çš„é—œéµå­—è¡¨ç¾ï¼ˆåŒ…æ‹¬è‡ªå·±çš„ç¶²ç«™ä¹‹é–“ï¼‰**")

                col1, col2 = st.columns(2)
                with col1:
                    site_a = st.selectbox(
                        "é¸æ“‡ç¶²ç«™ A", 
                        all_sites_in_record, 
                        key="compete_site_a",
                        help="é¸æ“‡ç¬¬ä¸€å€‹ç¶²ç«™é€²è¡Œæ¯”è¼ƒ"
                    )
                with col2:
                    # éæ¿¾æ‰å·²é¸çš„ç¶²ç«™ A
                    site_b_options = [s for s in all_sites_in_record if normalize_domain(s) != normalize_domain(site_a)]
                    site_b = st.selectbox(
                        "é¸æ“‡ç¶²ç«™ B", 
                        site_b_options if site_b_options else all_sites_in_record, 
                        key="compete_site_b",
                        help="é¸æ“‡ç¬¬äºŒå€‹ç¶²ç«™é€²è¡Œæ¯”è¼ƒ"
                    )

                if site_a and site_b and normalize_domain(site_a) != normalize_domain(site_b):
                    # é¡¯ç¤ºæ¯”è¼ƒé¡å‹
                    site_a_type = "ğŸ  æˆ‘çš„ç¶²ç«™" if site_a in tracked_my_sites else "ğŸ¯ ç«¶çˆ­å°æ‰‹"
                    site_b_type = "ğŸ  æˆ‘çš„ç¶²ç«™" if site_b in tracked_my_sites else "ğŸ¯ ç«¶çˆ­å°æ‰‹"
                    
                    st.markdown(f"""
                    **æ¯”è¼ƒï¼š** {site_a_type} `{site_a}` **vs** {site_b_type} `{site_b}`
                    """)
                    
                    # åˆ†æç«¶çˆ­æƒ…æ³ï¼ˆå‚³å…¥ keyword_order_map ä»¥ä¿æŒé †åºï¼‰
                    competition = analyze_keyword_competition(rankings, site_a, site_b, keyword_order_map)
                    
                    winning = competition["winning"]
                    losing = competition["losing"]
                    only_a = competition["only_a"]
                    only_b = competition["only_b"]
                    neither = competition["neither"]

                    # é¡¯ç¤ºçµ±è¨ˆ - ä¿®æ”¹æ¨™ç±¤é¡¯ç¤ºå°æ‡‰çš„ç¶²ç«™åç¨±
                    stat_cols = st.columns(5)
                    
                    # æˆªæ–·ç¶²ç«™åç¨±ä»¥é©æ‡‰é¡¯ç¤º
                    site_a_short = site_a[:15] + "..." if len(site_a) > 15 else site_a
                    site_b_short = site_b[:15] + "..." if len(site_b) > 15 else site_b
                    
                    with stat_cols[0]:
                        st.metric(f"ğŸ† {site_a_short} è´", len(winning))
                    with stat_cols[1]:
                        st.metric(f"ğŸ˜¢ {site_a_short} è¼¸", len(losing))
                    with stat_cols[2]:
                        st.metric(f"âœ… åªæœ‰ {site_a_short}", len(only_a))
                    with stat_cols[3]:
                        st.metric(f"âš ï¸ åªæœ‰ {site_b_short}", len(only_b))
                    with stat_cols[4]:
                        st.metric("âŒ éƒ½æ²’æ’å", len(neither))

                    # Tab æ¨™ç±¤ä¹Ÿé¡¯ç¤ºç¶²ç«™åç¨±
                    compete_tabs = st.tabs([
                        f"ğŸ† {site_a_short} è´ ({len(winning)})",
                        f"ğŸ˜¢ {site_a_short} è¼¸ ({len(losing)})",
                        f"âœ… åªæœ‰ {site_a_short} ({len(only_a)})",
                        f"âš ï¸ åªæœ‰ {site_b_short} ({len(only_b)})",
                        f"âŒ éƒ½æ²’æ’å ({len(neither)})"
                    ])

                    with compete_tabs[0]:
                        if winning:
                            st.success(f"ğŸ‰ ä»¥ä¸‹ {len(winning)} å€‹é—œéµå­— **{site_a}** æ’åé ˜å…ˆï¼")
                            win_data = []
                            for item in winning:
                                win_data.append({
                                    "é—œéµå­—": item["keyword"],
                                    f"{site_a} æ’å": item["rank_a"],
                                    f"{site_b} æ’å": item["rank_b"],
                                    "å„ªå‹¢": item["rank_b"] - item["rank_a"]
                                })
                            win_df = pd.DataFrame(win_data)
                            st.dataframe(win_df, use_container_width=True, hide_index=True)
                        else:
                            st.info(f"**{site_a}** æ²’æœ‰é ˜å…ˆçš„é—œéµå­—")

                    with compete_tabs[1]:
                        if losing:
                            st.error(f"âš ï¸ ä»¥ä¸‹ {len(losing)} å€‹é—œéµå­— **{site_a}** éœ€è¦åŠ å¼·ï¼")
                            lose_data = []
                            for item in losing:
                                lose_data.append({
                                    "é—œéµå­—": item["keyword"],
                                    f"{site_a} æ’å": item["rank_a"],
                                    f"{site_b} æ’å": item["rank_b"],
                                    "è½å¾Œ": item["rank_a"] - item["rank_b"]
                                })
                            lose_df = pd.DataFrame(lose_data)
                            st.dataframe(lose_df, use_container_width=True, hide_index=True)
                        else:
                            st.success(f"**{site_a}** æ²’æœ‰è½å¾Œçš„é—œéµå­—ï¼")

                    with compete_tabs[2]:
                        if only_a:
                            st.success(f"âœ… ä»¥ä¸‹ {len(only_a)} å€‹é—œéµå­—åªæœ‰ **{site_a}** ä¸Šæ¦œï¼")
                            only_a_data = []
                            for item in only_a:
                                only_a_data.append({
                                    "é—œéµå­—": item["keyword"],
                                    f"{site_a} æ’å": item["rank_a"]
                                })
                            only_a_df = pd.DataFrame(only_a_data)
                            st.dataframe(only_a_df, use_container_width=True, hide_index=True)
                        else:
                            st.info(f"**{site_a}** æ²’æœ‰ç¨ä½”çš„é—œéµå­—")

                    with compete_tabs[3]:
                        if only_b:
                            st.warning(f"âš ï¸ ä»¥ä¸‹ {len(only_b)} å€‹é—œéµå­—åªæœ‰ **{site_b}** ä¸Šæ¦œï¼")
                            only_b_data = []
                            for item in only_b:
                                only_b_data.append({
                                    "é—œéµå­—": item["keyword"],
                                    f"{site_b} æ’å": item["rank_b"]
                                })
                            only_b_df = pd.DataFrame(only_b_data)
                            st.dataframe(only_b_df, use_container_width=True, hide_index=True)
                        else:
                            st.success(f"**{site_b}** æ²’æœ‰ç¨ä½”çš„é—œéµå­—ï¼")

                    with compete_tabs[4]:
                        if neither:
                            st.info(f"ä»¥ä¸‹ {len(neither)} å€‹é—œéµå­—é›™æ–¹éƒ½æ²’æ’åï¼š")
                            neither_cols = st.columns(3)
                            for idx, item in enumerate(neither):
                                with neither_cols[idx % 3]:
                                    st.markdown(f"â€¢ {item['keyword']}")
                        else:
                            st.info("æ‰€æœ‰é—œéµå­—è‡³å°‘æœ‰ä¸€æ–¹æœ‰æ’å")

                elif site_a and site_b:
                    st.warning("âš ï¸ è«‹é¸æ“‡å…©å€‹ä¸åŒçš„ç¶²ç«™é€²è¡Œæ¯”è¼ƒ")

                st.markdown("---")

                # ============ 2. æŸ¥çœ‹å„ç¶²ç«™è©³ç´°é—œéµå­—ï¼ˆç¬¬äºŒä½ï¼‰============

                st.markdown("### ğŸ” æŸ¥çœ‹å„ç¶²ç«™è©³ç´°é—œéµå­—")

                selected_site_detail = st.selectbox(
                    "é¸æ“‡ç¶²ç«™æŸ¥çœ‹è©³ç´°",
                    all_sites_in_record,
                    key="detail_site_select"
                )

                if selected_site_detail:
                    details = analyze_site_keywords_detail(rankings, selected_site_detail, analysis_warning_threshold, keyword_order_map)
                    is_my_site = selected_site_detail in tracked_my_sites
                    site_type = "ğŸ  æˆ‘çš„ç¶²ç«™" if is_my_site else "ğŸ¯ ç«¶çˆ­å°æ‰‹"

                    st.markdown(f"#### {site_type}: **{selected_site_detail}**")

                    # çµ±è¨ˆæ•¸æ“š
                    stat_cols = st.columns(6)
                    categories_info = [
                        ("ğŸ† å‰3å", len(details['top3']), "#10B981"),
                        ("ğŸ“„ é¦–é 4-10", len(details['top10']), "#3B82F6"),
                        ("ğŸ“‘ ç¬¬2é ", len(details['top20']), "#F59E0B"),
                        ("ğŸ“‹ ç¬¬3é ", len(details['top30']), "#8B5CF6"),
                        (f"âš ï¸ >{analysis_warning_threshold}å", len(details['warning']), "#EF4444"),
                        ("âŒ æœªä¸Šæ¦œ", len(details['na']), "#6B7280")
                    ]

                    for i, (label, count, color) in enumerate(categories_info):
                        with stat_cols[i]:
                            st.markdown(f"""
                            <div style="text-align: center; padding: 0.5rem; background: white; border-radius: 8px; border-left: 3px solid {color};">
                                <div style="font-size: 1.5rem; font-weight: bold; color: {color};">{count}</div>
                                <div style="font-size: 0.75rem; color: #666;">{label}</div>
                            </div>
                            """, unsafe_allow_html=True)

                    detail_tabs = st.tabs([
                        f"ğŸ† å‰3å ({len(details['top3'])})",
                        f"ğŸ“„ é¦–é 4-10 ({len(details['top10'])})",
                        f"ğŸ“‘ ç¬¬2é 11-20 ({len(details['top20'])})",
                        f"ğŸ“‹ ç¬¬3é 21-30 ({len(details['top30'])})",
                        f"âš ï¸ >{analysis_warning_threshold}å ({len(details['warning'])})",
                        f"âŒ æœªä¸Šæ¦œ ({len(details['na'])})"
                    ])

                    with detail_tabs[0]:
                        if details["top3"]:
                            for item in details["top3"]:
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨å‰3å")

                    with detail_tabs[1]:
                        if details["top10"]:
                            for item in details["top10"]:
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨é¦–é 4-10å")

                    with detail_tabs[2]:
                        if details["top20"]:
                            for item in details["top20"]:
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨ç¬¬2é ")

                    with detail_tabs[3]:
                        if details["top30"]:
                            for item in details["top30"]:
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.info("æ²’æœ‰é—œéµå­—åœ¨ç¬¬3é ")

                    with detail_tabs[4]:
                        if details["warning"]:
                            st.warning(f"âš ï¸ ä»¥ä¸‹ {len(details['warning'])} å€‹é—œéµå­—æ’åè¶…é {analysis_warning_threshold}ï¼š")
                            for item in details["warning"]:
                                st.markdown(f"**#{item['rank']}** - {item['keyword']}")
                        else:
                            st.success(f"âœ… æ²’æœ‰é—œéµå­—è¶…é {analysis_warning_threshold} åï¼")

                    with detail_tabs[5]:
                        if details["na"]:
                            st.error(f"âŒ ä»¥ä¸‹ {len(details['na'])} å€‹é—œéµå­—æœªä¸Šæ¦œï¼š")
                            # åˆ†åˆ—é¡¯ç¤º
                            na_cols = st.columns(3)
                            for idx, item in enumerate(details["na"]):
                                with na_cols[idx % 3]:
                                    st.markdown(f"â€¢ {item['keyword']}")
                        else:
                            st.success("âœ… æ‰€æœ‰é—œéµå­—éƒ½æœ‰æ’åï¼")

                st.markdown("---")

                # ============ 3. é—œéµå­—æ­·å²è®ŠåŒ–ï¼ˆç¬¬ä¸‰ä½ï¼‰============

                st.markdown("### ğŸ“ˆ é—œéµå­—æ’åæ­·å²è®ŠåŒ–")

                if len(history_records) >= 2:
                    # æ”¶é›†æ‰€æœ‰é—œéµå­—å’Œç¶²ç«™
                    all_keywords = set()
                    all_sites_history = set()
                    for record in history_records:
                        for item in record.get("rankings", []):
                            all_keywords.add(item.get("keyword"))
                        all_sites_history.update(record.get("my_sites", []))
                        all_sites_history.update(record.get("competitors", []))

                    # æ¨™æº–åŒ–ç¶²ç«™åˆ—è¡¨ï¼Œåˆä½µç›¸åŒç¶²åŸŸ
                    normalized_sites_map = {}  # normalized -> display name
                    for site in all_sites_history:
                        normalized = normalize_domain(site)
                        if normalized not in normalized_sites_map:
                            normalized_sites_map[normalized] = site

                    unique_sites_for_chart = list(normalized_sites_map.values())

                    col1, col2 = st.columns(2)
                    with col1:
                        # å¤šé¸é—œéµå­—
                        selected_keywords = st.multiselect(
                            "é¸æ“‡é—œéµå­—ï¼ˆå¯å¤šé¸ï¼‰",
                            sorted(list(all_keywords)),
                            default=sorted(list(all_keywords))[:3] if len(all_keywords) >= 3 else sorted(
                                list(all_keywords)),
                            key="analysis_keywords"
                        )
                    with col2:
                        selected_site_for_chart = st.selectbox(
                            "é¸æ“‡ç¶²ç«™",
                            sorted(unique_sites_for_chart),
                            key="analysis_site"
                        )

                    if selected_keywords and selected_site_for_chart:
                        # å»ºç«‹æ­·å²æ•¸æ“š
                        chart_data = []
                        selected_normalized = normalize_domain(selected_site_for_chart)

                        for record in history_records:
                            date = record.get("date", "")
                            time_str = record.get("time", "")
                            datetime_str = f"{date} {time_str}"

                            row = {"æ—¥æœŸ": datetime_str}
                            for item in record.get("rankings", []):
                                kw = item.get("keyword")
                                if kw in selected_keywords:
                                    # æŸ¥æ‰¾åŒ¹é…çš„ç¶²ç«™ï¼ˆæ¨™æº–åŒ–æ¯”å°ï¼‰
                                    rank = None
                                    for site_key in item.keys():
                                        if site_key != "keyword" and normalize_domain(site_key) == selected_normalized:
                                            rank = item.get(site_key)
                                            break
                                    row[kw] = rank
                            chart_data.append(row)

                        df_chart = pd.DataFrame(chart_data)
                        df_chart = df_chart.set_index("æ—¥æœŸ")

                        # é¡¯ç¤ºè©³ç´°æ•¸æ“šè¡¨ï¼ˆç”¨è¡¨æ ¼ä»£æ›¿åœ–è¡¨ï¼‰
                        st.markdown("#### ğŸ“Š æ’åè®ŠåŒ–æ•¸æ“š")
                        st.dataframe(df_chart.reset_index(), use_container_width=True)

                        # è¨ˆç®—æ¯å€‹é—œéµå­—çš„è®ŠåŒ–
                        st.markdown("#### ğŸ“ˆ æ’åè®ŠåŒ–çµ±è¨ˆ")

                        change_data = []
                        for kw in selected_keywords:
                            if kw in df_chart.columns:
                                values = df_chart[kw].dropna()
                                if len(values) >= 1:
                                    first_rank = values.iloc[0] if len(values) >= 1 else None
                                    last_rank = values.iloc[-1] if len(values) >= 1 else None

                                    if len(values) >= 2:
                                        change = first_rank - last_rank  # æ­£æ•¸è¡¨ç¤ºæ’åä¸Šå‡
                                        change_str = f"{'â†‘' if change > 0 else 'â†“' if change < 0 else 'â”€'}{abs(int(change))}" if change != 0 else "â”€"
                                    else:
                                        change_str = "â”€"

                                    best_rank = values.min()
                                    worst_rank = values.max()
                                    avg_rank = values.mean()

                                    change_data.append({
                                        "é—œéµå­—": kw,
                                        "é¦–æ¬¡æ’å": int(first_rank) if pd.notna(first_rank) else "N/A",
                                        "æœ€æ–°æ’å": int(last_rank) if pd.notna(last_rank) else "N/A",
                                        "è®ŠåŒ–": change_str,
                                        "æœ€ä½³æ’å": int(best_rank) if pd.notna(best_rank) else "N/A",
                                        "æœ€å·®æ’å": int(worst_rank) if pd.notna(worst_rank) else "N/A",
                                        "å¹³å‡æ’å": f"{avg_rank:.1f}" if pd.notna(avg_rank) else "N/A"
                                    })

                        if change_data:
                            df_change = pd.DataFrame(change_data)
                            st.dataframe(df_change, use_container_width=True, hide_index=True)
                else:
                    st.info("éœ€è¦è‡³å°‘2æ¬¡æŸ¥è©¢è¨˜éŒ„æ‰èƒ½é¡¯ç¤ºæ­·å²è®ŠåŒ–")

                st.markdown("---")

                # ============ 4. ç¶²ç«™æ’åæ¯”è¼ƒç¸½è¦½ï¼ˆç¬¬å››ä½ï¼‰============

                st.markdown("### âš”ï¸ ç¶²ç«™æ’åæ¯”è¼ƒç¸½è¦½")

                comparison_data = []

                for site in all_sites_in_record:
                    details = analyze_site_keywords_detail(rankings, site, analysis_warning_threshold, keyword_order_map)
                    
                    # è¨ˆç®—å¹³å‡æ’å
                    all_ranks = []
                    for cat in ["top3", "top10", "top20", "top30"]:
                        all_ranks.extend([item["rank"] for item in details[cat]])
                    
                    avg_rank = round(sum(all_ranks) / len(all_ranks), 1) if all_ranks else "N/A"
                    
                    comparison_data.append({
                        "ç¶²ç«™": site,
                        "é¡å‹": "ğŸ  æˆ‘çš„ç¶²ç«™" if site in tracked_my_sites else "ğŸ¯ ç«¶çˆ­å°æ‰‹",
                        "å‰3å": len(details["top3"]),
                        "é¦–é (4-10)": len(details["top10"]),
                        "ç¬¬2é (11-20)": len(details["top20"]),
                        "21-30å": len(details["top30"]),
                        f">{analysis_warning_threshold}å": len(details["warning"]),
                        "æœªä¸Šæ¦œ": len(details["na"]),
                        "å¹³å‡æ’å": avg_rank
                    })

                df_comparison = pd.DataFrame(comparison_data)


                def highlight_comparison(row):
                    if "æˆ‘çš„ç¶²ç«™" in row["é¡å‹"]:
                        return ['background-color: #EFF6FF'] * len(row)
                    else:
                        return ['background-color: #FFFBEB'] * len(row)


                styled_comparison = df_comparison.style.apply(highlight_comparison, axis=1)
                st.dataframe(styled_comparison, use_container_width=True, hide_index=True)

# ============ Tab 3: æ­·å²è¨˜éŒ„ï¼ˆç§»åˆ°æ•¸æ“šåˆ†æå¾Œé¢ï¼‰============

elif st.session_state.current_tab == 3:
    st.markdown("### ğŸ“œ æ­·å²è¨˜éŒ„")

    history_records = st.session_state.history.get("records", [])

    if not history_records:
        st.info("ğŸ“Š é‚„æ²’æœ‰æ­·å²è¨˜éŒ„ï¼Œè«‹å…ˆåŸ·è¡Œæ’åæŸ¥è©¢")
    else:
        st.markdown(f"**å…± {len(history_records)} æ¢è¨˜éŒ„**")

        # æ­·å²è¨˜éŒ„çš„è­¦å‘Šé–¾å€¼è¨­å®š
        st.markdown("---")
        history_warning_threshold = st.number_input(
            "âš ï¸ æ­·å²è¨˜éŒ„è­¦å‘Šé–¾å€¼",
            min_value=10,
            max_value=100,
            value=20,
            step=5,
            key="history_warning_threshold",
            help="ç”¨æ–¼æ­·å²è¨˜éŒ„ä¸­çš„æ’åé¡è‰²æ¨™ç¤º"
        )

        st.markdown("---")

        # é¡¯ç¤ºæ¯æ¢è¨˜éŒ„
        for i, record in enumerate(reversed(history_records)):
            record_idx = len(history_records) - 1 - i  # å¯¦éš›ç´¢å¼•
            record_date = record.get("date", "æœªçŸ¥")
            record_time = record.get("time", "")
            record_id = record.get("id", f"record_{i}")
            keyword_count = len(record.get("rankings", []))
            my_sites_count = len(record.get("my_sites", []))
            competitor_count = len(record.get("competitors", []))
            autocorrect_status = "é–‹" if record.get("autocorrect", False) else "é—œ"

            col1, col2, col3 = st.columns([4, 1, 1])

            with col1:
                expander_title = f"ğŸ“… {record_date} {record_time} | {keyword_count}å€‹é—œéµå­— | {my_sites_count}å€‹ç¶²ç«™ | {competitor_count}å€‹å°æ‰‹ | è‡ªå‹•æ ¡æ­£:{autocorrect_status}"

                with st.expander(expander_title, expanded=False):
                    # åŸºæœ¬è³‡è¨Š
                    info_col1, info_col2 = st.columns(2)
                    with info_col1:
                        st.markdown("**ğŸ  æŸ¥è©¢çš„ç¶²ç«™ï¼š**")
                        st.write(", ".join(record.get("my_sites", [])))
                    with info_col2:
                        st.markdown("**ğŸ¯ ç«¶çˆ­å°æ‰‹ï¼š**")
                        st.write(", ".join(record.get("competitors", [])))

                    st.markdown("**ğŸ”‘ é—œéµå­—ï¼š**")
                    st.write(", ".join(record.get("keywords", [])))

                    st.markdown("---")

                    # ä½¿ç”¨ç›¸åŒçš„è©³ç´°æ’åæ ¼å¼é¡¯ç¤º
                    st.markdown("### ğŸ“‹ è©³ç´°æ’å")

                    record_my_sites = record.get("my_sites", [])
                    record_competitors = record.get("competitors", [])
                    record_rankings = record.get("rankings", [])

                    st.markdown("""
                    **åœ–ä¾‹ï¼š** ğŸ”µ æˆ‘çš„ç¶²ç«™ï¼ˆè—è‰²ç³»ï¼‰| ğŸŸ  ç«¶çˆ­å°æ‰‹ï¼ˆæ©™è‰²ç³»ï¼‰| âš ï¸ ç´…è‰² = æ’å > {} | N/A = æœªä¸Šæ¦œ
                    """.format(history_warning_threshold))

                    # ç²å–å‰ä¸€æ¢è¨˜éŒ„ç”¨æ–¼æ¯”è¼ƒ
                    prev_rankings_dict = {}
                    if record_idx > 0:
                        prev_record = history_records[record_idx - 1]
                        for item in prev_record.get("rankings", []):
                            prev_rankings_dict[item.get("keyword")] = item

                    # å‰µå»ºå¸¶æ¨£å¼çš„è¡¨æ ¼
                    df_display, styled_df = create_styled_ranking_dataframe(
                        record_rankings,
                        record_my_sites,
                        record_competitors,
                        history_warning_threshold,
                        prev_rankings_dict
                    )

                    st.dataframe(styled_df, use_container_width=True, height=400)

                    # ç¶²ç«™æ’åç¸½è¦½
                    st.markdown("---")
                    st.markdown("### ğŸ“Š æ’ååˆ†ä½ˆç¸½è¦½")

                    all_record_sites = record_my_sites + record_competitors

                    summary_data = []
                    for site in all_record_sites:
                        analysis = analyze_site_rankings(record_rankings, site, history_warning_threshold)
                        summary_data.append({
                            "ç¶²ç«™": site,
                            "é¡å‹": "ğŸ " if site in record_my_sites else "ğŸ¯",
                            "å‰3å": len(analysis["top3"]),
                            "é¦–é (4-10)": len(analysis["top10"]),
                            "ç¬¬2é (11-20)": len(analysis["top20"]),
                            "ç¬¬3é (21-30)": len(analysis["top30"]),
                            f">{history_warning_threshold}å": len(analysis["warning"]),
                            "æœªä¸Šæ¦œ": len(analysis["na"])
                        })

                    if summary_data:
                        df_summary = pd.DataFrame(summary_data)
                        st.dataframe(df_summary, use_container_width=True, hide_index=True)

            with col2:
                excel_data = export_single_record(record)
                st.download_button(
                    label="ğŸ“¥ Excel",
                    data=excel_data,
                    file_name=f"serp_{record_date}_{record_time.replace(':', '')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"dl_excel_{record_id}_{i}"
                )

            with col3:
                json_data = json.dumps(record, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“¥ JSON",
                    data=json_data,
                    file_name=f"serp_{record_date}_{record_time.replace(':', '')}.json",
                    mime="application/json",
                    key=f"dl_json_{record_id}_{i}"
                )

        st.markdown("---")

        # è¶¨å‹¢åˆ†æ
        st.markdown("### ğŸ“ˆ æ’åè¶¨å‹¢")

        if len(history_records) >= 2:
            all_keywords = set()
            all_tracked_sites = set()
            for record in history_records:
                for item in record.get("rankings", []):
                    all_keywords.add(item.get("keyword"))
                all_tracked_sites.update(record.get("my_sites", []))
                all_tracked_sites.update(record.get("competitors", []))

            # æ¨™æº–åŒ–ç¶²ç«™åˆ—è¡¨
            normalized_sites_map = {}  # normalized -> display name
            for site in all_tracked_sites:
                normalized = normalize_domain(site)
                if normalized not in normalized_sites_map:
                    normalized_sites_map[normalized] = site

            unique_sites = list(normalized_sites_map.values())

            col1, col2 = st.columns(2)
            with col1:
                selected_keyword = st.selectbox("é¸æ“‡é—œéµå­—", sorted(list(all_keywords)), key="trend_keyword")
            with col2:
                selected_site = st.selectbox("é¸æ“‡ç¶²ç«™", sorted(unique_sites), key="trend_site")

            trend_data = []
            selected_normalized = normalize_domain(selected_site)

            for record in history_records:
                date = record.get("date", "æœªçŸ¥")
                time_str = record.get("time", "")
                for item in record.get("rankings", []):
                    if item.get("keyword") == selected_keyword:
                        # æŸ¥æ‰¾åŒ¹é…çš„ç¶²ç«™ï¼ˆæ¨™æº–åŒ–æ¯”å°ï¼‰
                        rank = None
                        for site_key in item.keys():
                            if site_key != "keyword" and normalize_domain(site_key) == selected_normalized:
                                rank = item.get(site_key)
                                break
                        trend_data.append({
                            "æ—¥æœŸæ™‚é–“": f"{date} {time_str}",
                            "æ’å": rank if rank else None
                        })
                        break

            if trend_data:
                df_trend = pd.DataFrame(trend_data).dropna()
                if not df_trend.empty:
                    st.line_chart(df_trend.set_index("æ—¥æœŸæ™‚é–“")["æ’å"])

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æœ€ä½³æ’å", int(df_trend["æ’å"].min()))
                    with col2:
                        st.metric("å¹³å‡æ’å", f"{df_trend['æ’å'].mean():.1f}")
                    with col3:
                        if len(df_trend) >= 2:
                            change = df_trend["æ’å"].iloc[0] - df_trend["æ’å"].iloc[-1]
                            st.metric("ç¸½è®ŠåŒ–", f"{change:+.0f}")
        else:
            st.info("éœ€è¦è‡³å°‘2æ¬¡è¨˜éŒ„æ‰èƒ½é¡¯ç¤ºè¶¨å‹¢")

# ============ Tab 4: ç®¡ç† ============

elif st.session_state.current_tab == 4:
    st.markdown("### âš™ï¸ æ•¸æ“šç®¡ç†")

    history_records = st.session_state.history.get("records", [])
    st.markdown(f"**ç¸½è¨˜éŒ„æ•¸ï¼š** {len(history_records)}")
    st.markdown(f"**é—œéµå­—çµ„æ•¸ï¼š** {len(st.session_state.keyword_groups)}")

    if st.session_state.debug_logs:
        st.markdown("#### ğŸ› æœ€è¿‘çš„èª¿è©¦æ—¥èªŒ")
        with st.expander("æŸ¥çœ‹æ—¥èªŒ"):
            for log in st.session_state.debug_logs[-30:]:
                st.text(log)

    st.markdown("---")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ğŸ“¤ åŒ¯å‡ºæ‰€æœ‰æ•¸æ“š")

        if history_records:
            json_data = json.dumps(st.session_state.history, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ åŒ¯å‡º JSON å‚™ä»½",
                data=json_data,
                file_name=f"serp_backup_{datetime.now().strftime('%Y%m%d')}.json",
                mime="application/json",
                use_container_width=True
            )

            all_records = []
            for record in history_records:
                date = record.get("date", "")
                time_str = record.get("time", "")
                for item in record.get("rankings", []):
                    all_records.append({"æ—¥æœŸ": date, "æ™‚é–“": time_str, **item})

            if all_records:
                output = BytesIO()
                pd.DataFrame(all_records).to_excel(output, index=False, engine="openpyxl")
                output.seek(0)
                st.download_button(
                    label="ğŸ“¥ åŒ¯å‡ºæ­·å² Excel",
                    data=output,
                    file_name=f"serp_history_{datetime.now().strftime('%Y%m%d')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )

    with col2:
        st.markdown("#### ğŸ“¥ åŒ¯å…¥æ•¸æ“š")

        uploaded_file = st.file_uploader("ä¸Šå‚³ JSON å‚™ä»½", type=["json"])
        if uploaded_file:
            try:
                imported_data = json.load(uploaded_file)
                if st.button("ç¢ºèªåŒ¯å…¥"):
                    st.session_state.history = imported_data
                    save_history(imported_data)
                    st.success("âœ… åŒ¯å…¥æˆåŠŸï¼")
                    st.rerun()
            except Exception as e:
                st.error(f"åŒ¯å…¥å¤±æ•—ï¼š{e}")

    st.markdown("---")
    st.markdown("#### ğŸ—‘ï¸ æ¸…é™¤æ•¸æ“š")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰è¨˜éŒ„", type="secondary"):
            st.session_state.history = {"records": [], "settings": {}}
            save_history(st.session_state.history)
            st.session_state.current_results = None
            st.success("âœ… å·²æ¸…é™¤")
            st.rerun()

    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰é—œéµå­—çµ„", type="secondary"):
            st.session_state.keyword_groups = {}
            save_keyword_groups({})
            st.success("âœ… å·²æ¸…é™¤æ‰€æœ‰é—œéµå­—çµ„")
            st.rerun()

# ============ é å°¾ ============

st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>ğŸš€ SEO æ’åè¿½è¹¤å·¥å…· Pro v2.6</p>
    <p style="font-size: 0.8rem;">æ™ºèƒ½åˆ†æ | ç«¶çˆ­å°æ‰‹è¿½è¹¤ | é—œéµå­—ç®¡ç† | Powered by Serper API</p>
</div>
""", unsafe_allow_html=True)
