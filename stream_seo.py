import streamlit as st
import requests
import pandas as pd
import time
import json
import os
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from io import BytesIO
import threading

# ============ é é¢è¨­å®š ============

st.set_page_config(
    page_title="SEO æ’åè¿½è¹¤å·¥å…· Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============ è‡ªè¨‚ CSS ============

st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
        text-align: center;
    }
    .stat-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        text-align: center;
        border-left: 4px solid #667eea;
    }
    .stat-number {
        font-size: 2.5rem;
        font-weight: bold;
        color: #667eea;
    }
    .stat-label {
        color: #666;
        font-size: 0.9rem;
    }
    .speed-badge {
        background: linear-gradient(135deg, #10B981, #059669);
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        margin-left: 10px;
    }
</style>
""", unsafe_allow_html=True)

# ============ æ•¸æ“šå„²å­˜åŠŸèƒ½ ============

DATA_FILE = "serp_history.json"


def load_history():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {"records": [], "settings": {}}
    return {"records": [], "settings": {}}


def save_history(data):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def add_record(history, record):
    record["timestamp"] = datetime.now().isoformat()
    record["date"] = datetime.now().strftime("%Y-%m-%d")
    history["records"].append(record)
    save_history(history)
    return history


# ============ é«˜é€Ÿç•°æ­¥æœå°‹å¼•æ“ ============

class FastSerpSearcher:
    """é«˜é€Ÿ SERP æœå°‹å™¨ - ä½¿ç”¨ç•°æ­¥ä¸¦è¡Œè«‹æ±‚"""

    def __init__(self, api_key, region="hk", lang="zh-tw", max_concurrent=20):
        self.api_key = api_key
        self.region = region
        self.lang = lang
        self.max_concurrent = max_concurrent  # æœ€å¤§ä¸¦ç™¼æ•¸
        self.results_cache = {}

    async def _fetch_single(self, session, keyword, page, semaphore):
        """ç•°æ­¥ç²å–å–®å€‹æœå°‹çµæœ"""
        async with semaphore:
            url = "https://google.serper.dev/search"
            payload = {
                "q": keyword,
                "gl": self.region,
                "hl": self.lang,
                "num": 10,
                "page": page
            }
            headers = {
                "X-API-KEY": self.api_key,
                "Content-Type": "application/json"
            }

            try:
                async with session.post(url, json=payload, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = data.get("organic", [])

                        # è¨ˆç®—å¯¦éš›æ’å
                        for result in results:
                            original_position = result.get("position", 0)
                            result["actual_rank"] = (page - 1) * 10 + original_position
                            result["page"] = page

                        return {
                            "keyword": keyword,
                            "page": page,
                            "results": results,
                            "success": True
                        }
                    else:
                        return {
                            "keyword": keyword,
                            "page": page,
                            "results": [],
                            "success": False,
                            "error": f"HTTP {response.status}"
                        }
            except Exception as e:
                return {
                    "keyword": keyword,
                    "page": page,
                    "results": [],
                    "success": False,
                    "error": str(e)
                }

    async def search_all_async(self, keywords, max_pages, progress_callback=None):
        """ç•°æ­¥ä¸¦è¡Œæœå°‹æ‰€æœ‰é—œéµå­—"""

        # å»ºç«‹æ‰€æœ‰ä»»å‹™
        tasks = []
        for keyword in keywords:
            for page in range(1, max_pages + 1):
                tasks.append((keyword, page))

        total_tasks = len(tasks)
        completed = 0

        # é™åˆ¶ä¸¦ç™¼æ•¸
        semaphore = asyncio.Semaphore(self.max_concurrent)

        # ä½¿ç”¨é€£æ¥æ± 
        connector = aiohttp.TCPConnector(limit=self.max_concurrent, limit_per_host=self.max_concurrent)
        timeout = aiohttp.ClientTimeout(total=30)

        all_results = {}

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # å»ºç«‹å”ç¨‹ä»»å‹™
            coroutines = [
                self._fetch_single(session, keyword, page, semaphore)
                for keyword, page in tasks
            ]

            # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™
            for coro in asyncio.as_completed(coroutines):
                result = await coro
                completed += 1

                keyword = result["keyword"]
                if keyword not in all_results:
                    all_results[keyword] = []

                if result["success"]:
                    all_results[keyword].extend(result["results"])

                # æ›´æ–°é€²åº¦
                if progress_callback:
                    progress_callback(completed, total_tasks, keyword)

        # æ’åºçµæœ
        for keyword in all_results:
            all_results[keyword].sort(key=lambda x: x.get("actual_rank", 999))

        return all_results

    def search_all(self, keywords, max_pages, progress_callback=None):
        """åŒæ­¥åŒ…è£å™¨ - åœ¨ Streamlit ä¸­èª¿ç”¨"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(
                self.search_all_async(keywords, max_pages, progress_callback)
            )
        finally:
            loop.close()


class ThreadedSerpSearcher:
    """å¤šç·šç¨‹ SERP æœå°‹å™¨ - å‚™ç”¨æ–¹æ¡ˆ"""

    def __init__(self, api_key, region="hk", lang="zh-tw", max_workers=20):
        self.api_key = api_key
        self.region = region
        self.lang = lang
        self.max_workers = max_workers
        self.session = requests.Session()

        # è¨­å®šé€£æ¥æ± 
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=max_workers,
            pool_maxsize=max_workers,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def _fetch_single(self, keyword, page):
        """ç²å–å–®å€‹æœå°‹çµæœ"""
        url = "https://google.serper.dev/search"
        payload = {
            "q": keyword,
            "gl": self.region,
            "hl": self.lang,
            "num": 10,
            "page": page
        }
        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json"
        }

        try:
            response = self.session.post(url, json=payload, headers=headers, timeout=15)
            if response.status_code == 200:
                data = response.json()
                results = data.get("organic", [])

                for result in results:
                    original_position = result.get("position", 0)
                    result["actual_rank"] = (page - 1) * 10 + original_position
                    result["page"] = page

                return {
                    "keyword": keyword,
                    "page": page,
                    "results": results,
                    "success": True
                }
        except Exception as e:
            pass

        return {
            "keyword": keyword,
            "page": page,
            "results": [],
            "success": False
        }

    def search_all(self, keywords, max_pages, progress_callback=None):
        """å¤šç·šç¨‹ä¸¦è¡Œæœå°‹"""

        # å»ºç«‹æ‰€æœ‰ä»»å‹™
        tasks = [(kw, page) for kw in keywords for page in range(1, max_pages + 1)]
        total_tasks = len(tasks)
        completed = 0

        all_results = {kw: [] for kw in keywords}

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_task = {
                executor.submit(self._fetch_single, kw, page): (kw, page)
                for kw, page in tasks
            }

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_task):
                result = future.result()
                completed += 1

                keyword = result["keyword"]
                if result["success"]:
                    all_results[keyword].extend(result["results"])

                if progress_callback:
                    progress_callback(completed, total_tasks, keyword)

        # æ’åº
        for keyword in all_results:
            all_results[keyword].sort(key=lambda x: x.get("actual_rank", 999))

        return all_results


# ============ æ’ååˆ†æå·¥å…· ============

def find_rankings(serp_results, sites):
    """å¾ SERP çµæœä¸­æ‰¾å‡ºæŒ‡å®šç¶²ç«™çš„æ’å"""
    rankings = []

    for keyword, results in serp_results.items():
        row = {"keyword": keyword}

        for site in sites:
            rank = None
            for result in results:
                if site in result.get("link", ""):
                    rank = result.get("actual_rank")
                    break
            row[site] = rank

        rankings.append(row)

    return rankings


# ============ åˆå§‹åŒ– Session State ============

if "history" not in st.session_state:
    st.session_state.history = load_history()

if "current_results" not in st.session_state:
    st.session_state.current_results = None

# ============ æ¨™é¡Œ ============

st.markdown("""
<div class="main-header">
    <h1>ğŸš€ SEO æ’åè¿½è¹¤å·¥å…· Pro <span class="speed-badge">âš¡ 50x é«˜é€Ÿç‰ˆ</span></h1>
    <p>ç•°æ­¥ä¸¦è¡Œæœå°‹ Â· è¿½è¹¤æ’åè®ŠåŒ– Â· åˆ†æç«¶çˆ­å°æ‰‹</p>
</div>
""", unsafe_allow_html=True)

# ============ å´é‚Šæ¬„è¨­å®š ============

with st.sidebar:
    st.markdown("## âš™ï¸ è¨­å®š")

    api_key = st.text_input(
        "ğŸ”‘ Serper API Key",
        type="password",
        help="åœ¨ serper.dev è¨»å†Šå–å¾—å…è²» API Key"
    )

    if api_key:
        st.success("âœ… API Key å·²è¨­å®š")
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥ API Key123")

    st.markdown("---")

    st.markdown("### ğŸ” æœå°‹è¨­å®š")

    col1, col2 = st.columns(2)
    with col1:
        search_region = st.selectbox(
            "åœ°å€",
            options=["hk", "tw", "sg", "my", "us", "uk"],
            format_func=lambda x: {
                "hk": "ğŸ‡­ğŸ‡° é¦™æ¸¯", "tw": "ğŸ‡¹ğŸ‡¼ å°ç£", "sg": "ğŸ‡¸ğŸ‡¬ æ–°åŠ å¡",
                "my": "ğŸ‡²ğŸ‡¾ é¦¬ä¾†è¥¿äº", "us": "ğŸ‡ºğŸ‡¸ ç¾åœ‹", "uk": "ğŸ‡¬ğŸ‡§ è‹±åœ‹"
            }[x]
        )

    with col2:
        search_lang = st.selectbox(
            "èªè¨€",
            options=["zh-tw", "zh-cn", "en"],
            format_func=lambda x: {"zh-tw": "ç¹é«”ä¸­æ–‡", "zh-cn": "ç®€ä½“ä¸­æ–‡", "en": "English"}[x]
        )

    max_pages = st.slider("ğŸ“„ çˆ¬å–é æ•¸", 1, 10, 5)

    st.markdown("---")

    # é€Ÿåº¦è¨­å®š
    st.markdown("### âš¡ é€Ÿåº¦è¨­å®š")

    search_method = st.radio(
        "æœå°‹æ–¹å¼",
        options=["async", "thread"],
        format_func=lambda x: {
            "async": "âš¡ ç•°æ­¥ (æœ€å¿«)",
            "thread": "ğŸ”„ å¤šç·šç¨‹ (ç©©å®š)"
        }[x],
        index=0
    )

    max_concurrent = st.slider(
        "æœ€å¤§ä¸¦ç™¼æ•¸",
        min_value=5,
        max_value=50,
        value=20,
        help="è¶Šé«˜è¶Šå¿«ï¼Œä½†å¯èƒ½è§¸ç™¼ API é™åˆ¶"
    )

    st.info(f"âš¡ é ä¼°é€Ÿåº¦æå‡: **{max_concurrent}x**")

    st.markdown("---")

    st.markdown("### ğŸ  æˆ‘çš„ç¶²ç«™")
    my_sites_input = st.text_area(
        "æ¯è¡Œä¸€å€‹ç¶²åŸŸ",
        value="example.com",
        height=100,
        key="my_sites"
    )
    my_sites = [s.strip() for s in my_sites_input.split("\n") if s.strip()]

    st.markdown("### ğŸ¯ ç«¶çˆ­å°æ‰‹")
    competitors_input = st.text_area(
        "æ¯è¡Œä¸€å€‹ç¶²åŸŸ",
        value="competitor1.com\ncompetitor2.com",
        height=100,
        key="competitors"
    )
    competitors = [s.strip() for s in competitors_input.split("\n") if s.strip()]

    st.markdown("---")

    st.markdown("### ğŸ“Š æ•¸æ“šçµ±è¨ˆ")
    total_records = len(st.session_state.history.get("records", []))
    st.metric("ç¸½è¨˜éŒ„æ•¸", total_records)

# ============ ä¸»è¦å€åŸŸ ============

tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” æ’åæŸ¥è©¢", "ğŸ“ˆ æ­·å²è¶¨å‹¢", "ğŸ“Š æ•¸æ“šåˆ†æ", "âš™ï¸ ç®¡ç†"])

# ============ Tab 1: æ’åæŸ¥è©¢ ============

with tab1:
    col_left, col_right = st.columns([2, 1])

    with col_left:
        st.markdown("### ğŸ“ è¼¸å…¥é—œéµå­—")
        keywords_input = st.text_area(
            "æ¯è¡Œä¸€å€‹é—œéµå­—",
            value="åˆ°æœƒ\nåˆ°æœƒæ¨ä»‹\næ´¾å°åˆ°æœƒ",
            height=200,
            key="keywords_input"
        )
        keywords = [k.strip() for k in keywords_input.split("\n") if k.strip()]

    with col_right:
        st.markdown("### ğŸ“‹ æŸ¥è©¢è³‡è¨Š")

        keyword_groups = {
            "åˆ°æœƒç›¸é—œ": ["åˆ°æœƒ", "åˆ°æœƒæ¨ä»‹", "åˆ°æœƒæœå‹™", "æ´¾å°åˆ°æœƒ", "å…¬å¸åˆ°æœƒ", "åˆ°æœƒå¤–è³£"],
            "é¤é£²ç›¸é—œ": ["catering", "å¤–è³£", "è¨‚é¤", "å®´æœƒ"],
        }

        selected_group = st.selectbox("å¿«é€ŸåŒ¯å…¥é—œéµå­—çµ„", ["é¸æ“‡..."] + list(keyword_groups.keys()))

        if selected_group != "é¸æ“‡..." and st.button("ğŸ“¥ åŒ¯å…¥"):
            st.session_state.keywords_input = "\n".join(keyword_groups[selected_group])
            st.rerun()

        st.markdown("---")

        total_requests = len(keywords) * max_pages
        st.markdown(f"**é—œéµå­—æ•¸é‡ï¼š** {len(keywords)}")
        st.markdown(f"**API è«‹æ±‚æ•¸ï¼š** {total_requests}")
        st.markdown(f"**ä¸¦ç™¼æ•¸ï¼š** {max_concurrent}")

        # é ä¼°æ™‚é–“
        if search_method == "async":
            est_time = max(total_requests / max_concurrent * 0.5, 2)
        else:
            est_time = max(total_requests / max_concurrent * 0.8, 3)

        st.markdown(f"**é ä¼°æ™‚é–“ï¼š** ~{est_time:.0f} ç§’")

        # å°æ¯”å‚³çµ±æ™‚é–“
        traditional_time = total_requests * 0.5
        speedup = traditional_time / est_time
        st.markdown(f"**é€Ÿåº¦æå‡ï¼š** ğŸš€ **{speedup:.0f}x**")

    st.markdown("---")

    col_btn1, col_btn2, col_btn3 = st.columns([1, 2, 1])
    with col_btn2:
        start_tracking = st.button("ğŸš€ é–‹å§‹é«˜é€Ÿè¿½è¹¤", type="primary", use_container_width=True)

    # ============ åŸ·è¡Œæœå°‹ ============

    if start_tracking:
        if not api_key:
            st.error("âŒ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ API Key")
            st.stop()

        if not keywords:
            st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
            st.stop()

        all_sites = my_sites + competitors
        if not all_sites:
            st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹è¦è¿½è¹¤çš„ç¶²ç«™")
            st.stop()

        # é€²åº¦é¡¯ç¤º
        progress_container = st.container()
        with progress_container:
            col1, col2 = st.columns([3, 1])
            with col1:
                progress_bar = st.progress(0)
                status_text = st.empty()
            with col2:
                time_display = st.empty()

        start_time = time.time()


        # é€²åº¦å›èª¿å‡½æ•¸
        def update_progress(completed, total, current_keyword):
            progress = completed / total
            progress_bar.progress(progress)
            elapsed = time.time() - start_time
            status_text.text(f"âš¡ å·²å®Œæˆ {completed}/{total} | ç›®å‰: {current_keyword}")
            time_display.markdown(f"**â±ï¸ {elapsed:.1f}s**")


        # é¸æ“‡æœå°‹æ–¹å¼
        if search_method == "async":
            searcher = FastSerpSearcher(
                api_key=api_key,
                region=search_region,
                lang=search_lang,
                max_concurrent=max_concurrent
            )
        else:
            searcher = ThreadedSerpSearcher(
                api_key=api_key,
                region=search_region,
                lang=search_lang,
                max_workers=max_concurrent
            )

        # åŸ·è¡Œæœå°‹
        serp_results = searcher.search_all(keywords, max_pages, update_progress)

        # è¨ˆç®—è€—æ™‚
        elapsed_time = time.time() - start_time

        # æå–æ’å
        all_rankings = find_rankings(serp_results, all_sites)

        # å®Œæˆ
        progress_bar.progress(1.0)
        status_text.text(f"âœ… å®Œæˆï¼å…±è€—æ™‚ {elapsed_time:.1f} ç§’")

        # é¡¯ç¤ºé€Ÿåº¦çµ±è¨ˆ
        total_requests = len(keywords) * max_pages
        traditional_time = total_requests * 0.5
        actual_speedup = traditional_time / elapsed_time

        st.success(f"""
        âœ… **æœå°‹å®Œæˆï¼**
        - ç¸½è«‹æ±‚æ•¸ï¼š{total_requests}
        - å¯¦éš›è€—æ™‚ï¼š{elapsed_time:.1f} ç§’
        - å‚³çµ±æ–¹å¼é ä¼°ï¼š{traditional_time:.0f} ç§’
        - ğŸš€ **å¯¦éš›åŠ é€Ÿï¼š{actual_speedup:.1f}x**
        """)

        # å„²å­˜çµæœ
        st.session_state.current_results = {
            "rankings": all_rankings,
            "serp_data": serp_results,
            "timestamp": datetime.now().isoformat(),
            "elapsed_time": elapsed_time
        }

        record = {
            "rankings": all_rankings,
            "my_sites": my_sites,
            "competitors": competitors,
            "region": search_region
        }
        st.session_state.history = add_record(st.session_state.history, record)

    # ============ é¡¯ç¤ºçµæœ ============

    if st.session_state.current_results:
        st.markdown("---")
        st.markdown("## ğŸ“Š æ’åçµæœ")

        results = st.session_state.current_results
        rankings = results["rankings"]
        all_sites = my_sites + competitors

        # ç²å–ä¸Šæ¬¡è¨˜éŒ„
        history_records = st.session_state.history.get("records", [])
        previous_rankings = {}
        if len(history_records) >= 2:
            prev_record = history_records[-2]
            for item in prev_record.get("rankings", []):
                kw = item.get("keyword")
                previous_rankings[kw] = item

        # çµ±è¨ˆå¡ç‰‡
        st.markdown("### ğŸ“ˆ ç¸½è¦½")

        col1, col2, col3, col4 = st.columns(4)

        top3_count = 0
        top10_count = 0
        improved_count = 0
        declined_count = 0

        for rank_data in rankings:
            for site in my_sites:
                rank = rank_data.get(site)
                if rank is not None:
                    if rank <= 3:
                        top3_count += 1
                    if rank <= 10:
                        top10_count += 1

                    kw = rank_data.get("keyword")
                    if kw in previous_rankings:
                        prev_rank = previous_rankings[kw].get(site)
                        if prev_rank is not None:
                            if rank < prev_rank:
                                improved_count += 1
                            elif rank > prev_rank:
                                declined_count += 1

        with col1:
            st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number">{top3_count}</div>
                <div class="stat-label">ğŸ† å‰ 3 å</div>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number">{top10_count}</div>
                <div class="stat-label">ğŸ“„ é¦–é æ’å</div>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number" style="color: #10B981;">{improved_count}</div>
                <div class="stat-label">ğŸ“ˆ æ’åä¸Šå‡</div>
            </div>
            """, unsafe_allow_html=True)

        with col4:
            st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number" style="color: #EF4444;">{declined_count}</div>
                <div class="stat-label">ğŸ“‰ æ’åä¸‹é™</div>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("---")

        # è©³ç´°è¡¨æ ¼
        st.markdown("### ğŸ“‹ è©³ç´°æ’å")

        display_data = []
        for rank_data in rankings:
            row = {"é—œéµå­—": rank_data.get("keyword")}

            for site in all_sites:
                rank = rank_data.get(site)
                kw = rank_data.get("keyword")

                change = ""
                if kw in previous_rankings:
                    prev_rank = previous_rankings[kw].get(site)
                    if prev_rank is not None and rank is not None:
                        diff = prev_rank - rank
                        if diff > 0:
                            change = f" â†‘{diff}"
                        elif diff < 0:
                            change = f" â†“{abs(diff)}"
                        else:
                            change = " â”€"

                row[site] = f"{rank}{change}" if rank is not None else "N/A"

            display_data.append(row)

        df_display = pd.DataFrame(display_data)


        def highlight_ranking(val):
            if "N/A" in str(val):
                return "background-color: #FEE2E2; color: #DC2626;"
            try:
                rank = int(str(val).split()[0])
                if rank <= 3:
                    return "background-color: #D1FAE5; color: #065F46; font-weight: bold;"
                elif rank <= 10:
                    return "background-color: #FEF3C7; color: #92400E;"
                elif rank <= 20:
                    return "background-color: #F3F4F6; color: #374151;"
            except:
                pass
            return ""


        st.dataframe(
            df_display.style.applymap(highlight_ranking, subset=all_sites),
            use_container_width=True,
            height=400
        )

        st.markdown("""
        **åœ–ä¾‹ï¼š** ğŸŸ¢ å‰ 3 å | ğŸŸ¡ é¦–é  (4-10) | âšª ç¬¬äºŒé  (11-20) | ğŸ”´ æœªä¸Šæ¦œ | â†‘ ä¸Šå‡ | â†“ ä¸‹é™
        """)

        # ä¸‹è¼‰
        st.markdown("---")


        def create_excel(rankings, serp_data, my_sites, competitors):
            output = BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                df_rankings = pd.DataFrame(rankings)
                df_rankings.to_excel(writer, sheet_name="æ’åç¸½è¦½", index=False)

                serp_records = []
                for keyword, results in serp_data.items():
                    for result in results:
                        serp_records.append({
                            "é—œéµå­—": keyword,
                            "æ’å": result.get("actual_rank"),
                            "æ¨™é¡Œ": result.get("title"),
                            "ç¶²å€": result.get("link"),
                            "æè¿°": result.get("snippet", "")[:200]
                        })
                df_serp = pd.DataFrame(serp_records)
                df_serp.to_excel(writer, sheet_name="å®Œæ•´SERP", index=False)

            output.seek(0)
            return output


        col_dl1, col_dl2 = st.columns(2)

        with col_dl1:
            excel_file = create_excel(rankings, results.get("serp_data", {}), my_sites, competitors)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š",
                data=excel_file,
                file_name=f"serp_ranking_{timestamp}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

        with col_dl2:
            csv_data = df_display.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ CSV",
                data=csv_data,
                file_name=f"serp_ranking_{timestamp}.csv",
                mime="text/csv",
                use_container_width=True
            )

# ============ Tab 2: æ­·å²è¶¨å‹¢ ============

with tab2:
    st.markdown("### ğŸ“ˆ æ’åè¶¨å‹¢åœ–")

    history_records = st.session_state.history.get("records", [])

    if len(history_records) < 2:
        st.info("ğŸ“Š éœ€è¦è‡³å°‘ 2 æ¬¡è¨˜éŒ„æ‰èƒ½é¡¯ç¤ºè¶¨å‹¢åœ–")
    else:
        all_keywords = set()
        all_tracked_sites = set()
        for record in history_records:
            for item in record.get("rankings", []):
                all_keywords.add(item.get("keyword"))
            all_tracked_sites.update(record.get("my_sites", []))
            all_tracked_sites.update(record.get("competitors", []))

        col1, col2 = st.columns(2)
        with col1:
            selected_keyword = st.selectbox("é¸æ“‡é—œéµå­—", sorted(list(all_keywords)))
        with col2:
            selected_site = st.selectbox("é¸æ“‡ç¶²ç«™", sorted(list(all_tracked_sites)))

        trend_data = []
        for record in history_records:
            date = record.get("date", "æœªçŸ¥")
            for item in record.get("rankings", []):
                if item.get("keyword") == selected_keyword:
                    rank = item.get(selected_site)
                    trend_data.append({"æ—¥æœŸ": date, "æ’å": rank if rank else None})
                    break

        if trend_data:
            df_trend = pd.DataFrame(trend_data).dropna()

            if not df_trend.empty:
                st.markdown(f"**ã€Œ{selected_keyword}ã€åœ¨ {selected_site} çš„æ’åè®ŠåŒ–**")
                st.line_chart(df_trend.set_index("æ—¥æœŸ")["æ’å"])
                st.dataframe(df_trend, use_container_width=True)

                st.markdown("#### ğŸ“Š çµ±è¨ˆæ‘˜è¦")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æœ€ä½³æ’å", int(df_trend["æ’å"].min()))
                with col2:
                    st.metric("æœ€å·®æ’å", int(df_trend["æ’å"].max()))
                with col3:
                    st.metric("å¹³å‡æ’å", f"{df_trend['æ’å'].mean():.1f}")
                with col4:
                    if len(df_trend) >= 2:
                        change = df_trend["æ’å"].iloc[0] - df_trend["æ’å"].iloc[-1]
                        st.metric("ç¸½è®ŠåŒ–", f"{change:+.0f}")

# ============ Tab 3: æ•¸æ“šåˆ†æ ============

with tab3:
    st.markdown("### ğŸ“Š SEO æ•¸æ“šåˆ†æ")

    history_records = st.session_state.history.get("records", [])

    if not history_records:
        st.info("ğŸ“Š é‚„æ²’æœ‰æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œæ’åæŸ¥è©¢")
    else:
        latest_record = history_records[-1]
        rankings = latest_record.get("rankings", [])
        tracked_my_sites = latest_record.get("my_sites", [])
        tracked_competitors = latest_record.get("competitors", [])

        if rankings:
            st.markdown("#### ğŸ† æ’ååˆ†ä½ˆï¼ˆæˆ‘çš„ç¶²ç«™ï¼‰")

            rank_distribution = {"å‰3å": 0, "é¦–é (4-10)": 0, "ç¬¬2é (11-20)": 0, "20åå¤–": 0, "æœªä¸Šæ¦œ": 0}

            for item in rankings:
                for site in tracked_my_sites:
                    rank = item.get(site)
                    if rank is None:
                        rank_distribution["æœªä¸Šæ¦œ"] += 1
                    elif rank <= 3:
                        rank_distribution["å‰3å"] += 1
                    elif rank <= 10:
                        rank_distribution["é¦–é (4-10)"] += 1
                    elif rank <= 20:
                        rank_distribution["ç¬¬2é (11-20)"] += 1
                    else:
                        rank_distribution["20åå¤–"] += 1

            st.bar_chart(pd.DataFrame([rank_distribution]).T)

            st.markdown("#### âš”ï¸ èˆ‡ç«¶çˆ­å°æ‰‹æ¯”è¼ƒ")

            comparison_data = []
            all_sites = tracked_my_sites + tracked_competitors

            for site in all_sites:
                site_stats = {"ç¶²ç«™": site, "å¹³å‡æ’å": 0, "é¦–é æ•¸": 0, "ç¸½é—œéµå­—": 0}
                ranks = []
                for item in rankings:
                    rank = item.get(site)
                    if rank is not None:
                        ranks.append(rank)
                        if rank <= 10:
                            site_stats["é¦–é æ•¸"] += 1

                if ranks:
                    site_stats["å¹³å‡æ’å"] = round(sum(ranks) / len(ranks), 1)
                    site_stats["ç¸½é—œéµå­—"] = len(ranks)

                site_stats["é¡å‹"] = "æˆ‘çš„ç¶²ç«™" if site in tracked_my_sites else "ç«¶çˆ­å°æ‰‹"
                comparison_data.append(site_stats)

            st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)

# ============ Tab 4: ç®¡ç† ============

with tab4:
    st.markdown("### âš™ï¸ æ•¸æ“šç®¡ç†")

    history_records = st.session_state.history.get("records", [])
    st.markdown(f"**ç¸½è¨˜éŒ„æ•¸ï¼š** {len(history_records)}")

    if history_records:
        st.markdown("#### ğŸ“œ æ­·å²è¨˜éŒ„")

        for i, record in enumerate(reversed(history_records[-10:])):
            date = record.get("date", "æœªçŸ¥")
            keyword_count = len(record.get("rankings", []))
            with st.expander(f"ğŸ“… {date} ({keyword_count} å€‹é—œéµå­—)"):
                st.json(record.get("rankings", []))

        st.markdown("---")

        col1, col2 = st.columns(2)

        with col1:
            json_data = json.dumps(st.session_state.history, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ åŒ¯å‡º JSON å‚™ä»½",
                data=json_data,
                file_name=f"serp_backup_{datetime.now().strftime('%Y%m%d')}.json",
                mime="application/json",
                use_container_width=True
            )

        with col2:
            all_records = []
            for record in history_records:
                date = record.get("date", "")
                for item in record.get("rankings", []):
                    all_records.append({"æ—¥æœŸ": date, **item})

            if all_records:
                output = BytesIO()
                pd.DataFrame(all_records).to_excel(output, index=False, engine="openpyxl")
                output.seek(0)
                st.download_button(
                    label="ğŸ“¥ åŒ¯å‡ºæ­·å² Excel",
                    data=output,
                    file_name=f"serp_history_{datetime.now().strftime('%Y%m%d')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )

        st.markdown("---")
        st.markdown("#### ğŸ—‘ï¸ æ¸…é™¤æ•¸æ“š")

        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰è¨˜éŒ„", type="secondary"):
            st.session_state.history = {"records": [], "settings": {}}
            save_history(st.session_state.history)
            st.success("âœ… å·²æ¸…é™¤")
            st.rerun()

    st.markdown("---")
    st.markdown("#### ğŸ“¤ åŒ¯å…¥æ•¸æ“š")

    uploaded_file = st.file_uploader("ä¸Šå‚³ JSON å‚™ä»½", type=["json"])
    if uploaded_file:
        try:
            imported_data = json.load(uploaded_file)
            if st.button("ç¢ºèªåŒ¯å…¥"):
                st.session_state.history = imported_data
                save_history(imported_data)
                st.success("âœ… åŒ¯å…¥æˆåŠŸï¼")
                st.rerun()
        except Exception as e:
            st.error(f"åŒ¯å…¥å¤±æ•—ï¼š{e}")

# ============ é å°¾ ============

st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>ğŸš€ SEO æ’åè¿½è¹¤å·¥å…· Pro123 <span class="speed-badge">âš¡ 50x é«˜é€Ÿç‰ˆ</span></p>
    <p style="font-size: 0.8rem;">Async + Multi-threading | Powered by Serper API</p>
</div>
""", unsafe_allow_html=True)

